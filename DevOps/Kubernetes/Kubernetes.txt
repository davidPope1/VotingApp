Kubernetes 

Containers

    -ca sa dam deploy la un app(care are un stack end to end), trebuie sa avem grija la toate compatibilitatile librariilor si dedendintelor dintre tehnologiile folosite(de ex web server nodejs, baza de date mongodb, orhcestrarion ansible si sistem de mesaje ca si redis)
    -trebuie sa fim atenti la compatibilitatea tuturor tehnologiilor de mai sus(redis, mongodb) cu versiunea sistemului de operare de baza folosit. Deci trebuie ca versiunea de OS sa fie compatibila cu versiunile tuturor celorlaltor tehnologii folosite.
    -trebuie sa fim atenti si la compatibilitatea dintre aceste servicii(nodejs, mongodb) si librariile si dependintele de pe OS. Pot fi situatii in care un serviciu are nevoie de o versiune a unei librarii dependente, pe cand alt serviciu avea nevoie de alta versiune.
    -de ex, atunci cand un developer nou intra in proiect, este extrem de dificil pt el sa-si seteze environmentul(extrem de multe instructiuni si sute de comenzi). Trebuiau sa se asigure ca folosesc sistemul de operare corect, versiunile corecte ale tuturor serviciilor.  
    -un developer e confortabil folosind un OS, iar ceilalti ar putea folosi altul, deci nu este garantat ca aplicatia pe care o construim o sa ruleze in acelasi fel pe toate OS-urile diferite.
    -si deci, toate astea ne fac viata foarte grea in developingu, buildingu si shippingu aplicatiei
    -deci, ne va trebui ceva care sa ne ajute cu problema compatibilitatii, ceva care sa ne permita sa modificam sau sa schimbam aceste componente, fara sa le afecteze pe celelalte componente, ba chiar sa modificam sistemu de operare daca avem nevoie
    -Docker este raspunsul la aceasta problema. Cu el, putem rula fiecare componenta intr-un container separat cu propriile lui librarii si dependinte. Toate pe acelasi VM si OS, dar cu containere(environmenturi) separate.
    -a trebuit sa builduim configuratia Docker o singura data si toti developerii pot acum sa sa inceapa cu o singura comanda docker run, necontand sistemul de operare pe care ei ruleaza.
    -tot ce au avut nevoie sa faca a fost sa se asigure ca au Docker instalat pe sistemele lor 
    -containerele sunt environmenturi complet izolate. Pot avea propriile procese sau servicii, propriile lor interfete network, propriile lor mount-uri, exact ca si VM-urile, doar ca ele toate sharuiesc acelasi kernel de sistem de operare.
    -containerele exista cu mult inainte docker. Tipuri de containere sunt LXC, LXD, FS, etc.. . Docker foloseste containerele LXC.
    -setarea acestor containere este grea, deoarece ele sunt foarte low level si aici Docker ofera un tool high level cu multe functionalitati puternice, facandu-l foarte usor de folosit pt end useri ca noi.
    -daca ne uitam la sisteme de operare ca ubuntu, defora, suse, centOS, ele toate constau in doua lucruri: un OS Kernel si un set de software 
    -kerenul sistemului de operare este responsabil sa interactioneze cu hardwareu din spate, in timp ce OS Kernelu ramane acelasi, adica Linux in acest caz. Software-ul de deasupra face aceste sisteme de operare diferite.
    -acest software poate consta in o interfata user diferita, drivere, compilere, file managers, developer tools, etc...
    -deci avem un kernel de linux comun shareuit pe toate sistemele de operare si un software custom care diferentiaza sistemele de operare intre ele.
    -am zis mai sus ca containerele Docker shareuiesc kernelul din spate. Dar ce inseamna defapt?
    -sa zicem ca avem un sistem ca ubuntu OS cu Docker instalat pe el. Docker poate rula orice flavor de OS pe el atat timp cand sunt toate bazate pe acelasi kernel, in cazu nostru, linux.
    -daca sistemul de operare din spate e ubuntu, Docker poate rula un container bazat pe o alta distributie ca debian, fedora, suse sau centOS.
    -fiecare container Docker are doar softwareu aditional care face aceste sisteme de operare diferite si Docker foloseste kernelu din spatele Docker Host-ului care merge cu toate sistemele de operare mentionate anterior.
    -un OS care nu impartaseste acelaso kernel ca si osurile de mai sus este Windows.
    -si deci, nu vom putea rula un container bazat pe windows pe un docker host care are linux os pe el. Pt asta avem nevoie de docker pe un server windows. 
    -ne putem pune intrebarea daca nu e un dezavantaj faptul ca nu putem rula alt kernel pe osu hostului. 
    -Raspunsu e nu, deoarece spre deosebire de hypervisors, Docker nu e facut sa virtualizeze si sa ruleze diferite sisteme de operare si kernel-uri pe acelasi hardware.
    -scopul principal al lui docker este sa containerizeze aplicatii si sa lea dea ship si run.
    -asta de aduce la diferentele dintre VM-uri si containere 
    -Docker: avem infrastructura hardware de baza, apoi sistemul de operare si Docker instalat pe acesta. Docker poate apoi sa menegiuiasca singur containerele care au librariile si dependintele si dupa aplicatia.
    -VM: avem harwareu de baza pe care e instalat OSu, apoi Hypervisorul ca ESX sau o virtualizare de vreun fel si apoi VM-urile. Fiecare VM are propriul ei sistem de operare in ea, apoi dependintele si apoi aplicatia.
    -acest overhead al VM-urilor cauzeaza o utilizare mai mare a resurselor din spate, pt ca sunt mai multe sisteme de operare virtuale si kernele care sunt running.
    -VM-urile consuma de asemenea, mai mult disk space, pt ca fiecare VM este heavy si are de obicei sizeu in giga bytes. 
    -pe cand containerele Docker sunt lightweight si au, de obicei, sizeu in mega bytes. 
    -de asta, containerele Docker booteaza mai rapid, de obicei, in cateva secunde, pe cand masinile virutale, dupa cum stim, au nevoie de cateva minute sa porneasca, pt ca trebuie sa booteze intregul sistem de operare.
    -de notat este si faptul ca Docker are mai putina izolare, deoarece mai multe resurse sunt impartasite intre containere ca si Kernelu, pe cand VM-urile au izolare completa unul de celalalt. 
    -deoarece VM-urile nu se bazeaza pe sistemu de operare sau kernelu din spate, putem avea diferite tipuri de sisteme de operare ca si linux based sau windows based pe acelasi hyervisor, ceea ce nu e posibil pe un singur host docker.

    -sunt multe versiuni containerizate ale aplicatiilor gata facute si disponibile azi. Deci majoritatea organizatiilor au produsele containerizate si disponibile intr-un public Docker registry numit Docker Hub sau Docker Store.
    -de ex, putem gasi imagini alte celor mai des folosite sisteme de operare, baze de date si alte servicii si tool-uri.
    -indata ce identificam imaginile de care avem nevoie si instalam Docker pe Hostu nostru, sa pornim un stack de aplicatie e atat de usor ca rularea comenzii docker run cu numele imaginii.
    -in cazul de fata, daca dam run la comanda: $ docker run ansible , va porni o instanta de Ansible pe docker host. similar, pornim instante de mongodb, redis, nodejs cu comanda docker run <nodejs sau redis ...>
    -cand rulam nodejs, doar pointam la locatia repoului cu codul pe host. Daca avem nevoie sa rulam mai multe instante ale web serviceului, efectiv adaugam cate instante avem nevoie(rulam comanda docker run de mai multe ori) si configuram un fel de load balancer in fata.
    -in caz ca da fail o instanta, o distrugem si dam launch la alta. Sunt alte solutii disponibile pt a avea grija de astfel de cazuri la care o sa ne uitam mai tz in acest curs.
    
    -am tot vb de imagini si containere, sa intelegem diferenta dintre cele doua:
    -o imagine este un package sau un template, fix ca si un template de VM cu care am fi putut lucrat in lumea virtualizarii.
    -o imagine este folosita sa creeze unul sau mai multe containere. 
    -containerele pornesc instante ale imaginilor care sunt izolate si au propriile lor environmenturi si seturi de procese.
    -multe produse au fost deja dockerizate. In caz ca nu gasim ce avem nevoie, putem sa creeam noi o imagine si sa ii dam push pe repou docker hub, facand-o disponibila publicului larg.

    -daca privim din perspectiva, developerii fac aplicatii pe care le predau echipelor Ops sa le dea deploy si sa le menegiuiasca in environmenturi de productie.
    -ei fac asta provizionand un set de instructiuni ca de ex informatii despre cum trebuie setat hostul, ce prerequisites trebuie instalate pe host si cum dependitnele sunt configurate, etc.. .
    -echipa Ops foloseste acest ghid to set up the app. Deoarece echipa Ops nu a creeat aplicatia, se chinuie sa ii faca set-up. Cand se lovesc de o problema, lucreaza cu developerii sa o rezolve.
    -cu docker, o mare parte a muncii implicata sa dea set up la infrastructura este acum in mainile developerilor in forma Dockerfile ului. 
    -ghidul pe care developerii l au facut mai devreme, sa configureze(set-up) infrastructura, poate acuma fi pus usor intr-un Dockerfile sa creeze o imagine pt aplicatii 
    -imaginea asta poate rula acum pe orice platforma de container si este garantata sa ruleze in acelasi fel peste tot. 
    -deci, echipa Ops poate acuma sa foloseasca imaginea efectiv sa dea deploy la aplicatie. Deoarece imaginea mergea deja cand developerul i-a dat build si Ops team nu o modifica, continua sa mearga in acelasi fel cand este deployed in productie.
    
Container Orchestration

    -acum avem apicatia noastra impachetata intr-un container(daca avem o aplicatie cu back front si db, vom avea 3 containere)
    -acum trebuie sa vedem cum lansam aplciatia in productie, daca aplictia noastra e bazata pe alte containere gen baza de date sau serviciu de mesaje sau alte servicii backend?
    -daca numarul de useri creste si trebuie sa dam scale la aplicatie(adica sa pornim mai multe instante) sau loadul scade(userii scad) si trebuie sa dam scale down la aplicatie(sa mai oprim din instante ca s prea multe pt prea putini useri si nu e eficitent)
    -ca sa dam enable la aceste functionalitati, avem nevoie de o platforma underlying cum un set de resurse 
    -platforma trebuie sa orchstreze conectivitatea dintre containere si automat sa dea scale up sau down inn functie de load 
    -procesul care automat da deploy si menegiuieste containerele e cunoscut ca si orchestrarea containerelor 

    -kubernetes este o tehnologie de orchestrare a containerelor 
    -mai sunt si altele disponibile, ca si Docker Swarm de la docker, Kubernetes de la google si Mesos de la Apache
    -in timp ce docker swarm este foarte usor de instalat condigurat si pornit, duce lipsa de niste feauture uri de autoscaling pt aplicatii mai complexe
    -pe de alta parte, Mesos este destul de greu de setat si pornit, dar ofera multe opriuni pt a customiza deployment-urile si suporta deployment de arhitecturi complexe 
    -Kubernetes e acu suportat pe toti providerii publici de cloud ca si GCP, Azure si AWS 

    -sunt multe avantaje daca folosim orchestrarea containerelor: 
    -aplicatia noastra este highly available din moment ce fail-urile harware nu ne dau jos aplicatia pt ca avem multe instante ale aplicatiei care ruleaza pe diferite noduri 
    -traficul userilor este load balanced pe mai multe containere. Cand cererea creste, se da deploy la mai multe instante ale aplcatiei instant, integrate perfect si putem face asta la nivel de serviciu 
    -cand nu mai avem resurse hardware, dam scale up sau down la nr de noduri fara sa trebuiasca sa oprim aplicatia
    -putem face toate astea usor cu un set de file uri declarative de configurare a obiectelor(a set of declarative object configuration files)
    
    -si asta e kubernetes. Este o tehnologie de orchestrare a containerelor folosita pt a orchestra deployment-ul si managementul a sute sau mii de containere intr-un clustered environment(mediu organizat)

Kubernetes Arhitecture 

    -un node este un machine(fizic sau virtual) pe care este kubernetes instalat 
    -un node este un worker machine si aici containerele vor fi pornite de kubernetes(este ca un docker host). Este sinonim cu minions 
    -daca un node da fail, atunci aplicatia noastra va da fail si ea, deci trebuie sa avem mai mult de un node
    -un cluster este un set de noduri grupate impreuna. Astfel, chiar daca un node da fail, avem aplicatia accesibila de pe celelalte noduri 
    -de asemenea, sa avem mai multe noduri ne ajuta si in impartirea load-ului 
    -pentru managementul clusterului este responsabil nodul master
    -pe acesta sunt stocate informatiile despre nodurile worker(nodes) din cluster 
    -de aici sunt monitorizate nodurile 
    -aici este configurat modul in care mutam workload ul unui nod, cand acesta da fail, altui nod 
    -masterul este un nod cu kubernetes instalat pe el si configurat ca si master, care supervizeaza nodurile din cluster si este responsabil de orchestrarea proriu-zisa a acestora 
    
    -cand instalam kubernetes pe un sistem, instalam defapt urmatoarele componente: un server API, un service etcd, un service kubelet, un container runetime, controllere si schedulere 
    -serverul de API este ca un frontned pt kubernetes. Userii, device urile de management, interfetele linii de comanda, toate comunica cu serverul API pt a interactiona cu clusterul kubernetes 
    -etcd este un distribuited, reliable key value store folosit de kubernetes sa stocheze toate informatiile folosite sa menegiuiasca clusterul 
    -cand avem mai multe noduri si mai multe master uri in cluster, etcd stocheaza toate informatiile referitoare la acestea pe toate nodurile din cluster intr un mod distribuit 
    -etcd este responsabil pt implementarea lock urilor prin cluster sa ne asiguram ca nu exista conflicte intre mastere 
    -Schedulerul este pt distribuirea workului sau a containerelor pe mai multe noduri. Cauta noduri noi creeate si le asigneaza containere.
    -Controllerele sunt creierele din spatele orchestrarii. Sunt responsabile de a observa si a raspunde cand nodurile, containerele sau endpoint urile se opresc(dau fail)
    -controllerele iau decizia de a porni containere noi in astfel de cazuri.
    -Container Runetime este softul din spate care e folosit sa porneasca containere, in cazul nostru s-a intamplat sa fie Docker. Dar sunt si alte optiuni, de asemenea
    -Kubelet este agentul care ruleaza pe fiecare nod din cluster. Agentul este responsabil sa se asigure ca containerele ruleaza pe noduri cum ne asteptam(as expected). 
    -pe nodurile worker sunt hostate containerele docker de ex. Pt a rula containere Docker pe un sistem, avem nevoie de Container Runetime instalat, in cazu nostru e Docker.
    -nodul Master are Kube API Server si asta il face sa fie master
    -la fel, nodurile worker au Kubelet Agent instalat, care interactioneza cu masterul pt a-i furniza informatii de sanantate ale nodului worker si sa faca actiuni cerute de master pe nodurile worker
    -toate informatiile obtinute sunt stocate intr-un key value store pe master. Key Value store este vazat pe framework ul etcd
    -masterul are de asemenea Controllerul si Scheduleru, pe langa alte componente 

    -kubectl(Kube Control este o utilitate in command line) este folosit sa dea deploy si manage la aplicatii pe un cluster kubernetes. Pt a lua informatii despre cluster, sau statusul nodurilor din cluster si sa menegiuiasca si alte lucruri 

    -comanda kubectl run este folosita sa dea deploy la o aplciatie pe cluster 

    $ kubectl run hello-minicube

    -comanda kubectl cluster-info este folosita pt a vedea informatii despre cluster

    $ kubectl cluster-info

    -comanda kubectl gety nodes e folosita pt a lista toate nodurile care sunt in cluster 

    $ kubectl get nodes 

    -ca sa rulam sute de instante ale aplicatiei noastre(de ex, my-web-app cu imaginea my-web-app) pe sute de noduri, avem nevoie de o singura comanda kubernetes ca asta:

    $ kubectl run my-web-app --image=my-web-app --replicas=100

ContainerD  

    -ContainerD care are ca si command line utility Ctr, care e folosita pt debugging si are putine feature-uri si deci, nu este user friendly. Aceasta face call-uri direct la API 
    -de ex, poate sa dea pull la imagini: $ ctr images pull docker.io/library/redis:alpine , trebuie specificat linkul complet al imaginii 
    -pt a da run la un container: ctr run docker.io/library/redis:alpine redis 
    -dar cum am mentionat anterior, acest tool este facut in special pt debugging si nu e de folosit pt a porni sau menegiui containere intr-un production environment 

    -o alternativa mai buna recomandata este Nerd Control Tool sau Nerd Ctl Tool 
    -este un comand line tool foarte similara cu Docker. Este ca un Docker CLI pt ContainerD 
    -suporta toate optiunile pe care le suporta si Docker si cele mai noi feature uri din ContainerD, de ex: Ecrypted container images, Lazy Pulling, P2P image distribution, Image signing and verifying, Namespaces in  Kubernetes 
    -deci, nerd coontrol tool merge foarte similar ca Docker CLI, deci in comanda, in loc de Docker, punem nerdctrl si putem rula aproape toate comenzile docker care interactioneaza cu containere astfel: 
    $ docker run --name=redis redis:alpine  ->  $ nerdctl run --name=redis redis:alpine     //pt a rula un cotnainer 
    $ docker run --name=webserver -p=80:80 -d nginx  ->  $ nerdctl run --name=webserver -p=80:80 -d nginx     //pt a rula un cotnainer cu port mapping 

    -CRI Control tool este alta Command Line Utility care e folosita sa interactioneze cu container runtimes compatibile cu CRI(CRI compatible container runtime) ca si ContainerD, Rocket si altele 
    -este un fel de interactiune din perspectiva Kubernetes care e developed si maintained de comunitatea kubernetes 
    -mai devreme am vb de CTR si NerdControlTool care au fost facute de comunitatea ContainerD specific pt a functiona cu ContainerD 
    -acest tool este facut de kubernetes pt a funciona cu mai multe container runtimes 
    -trebuie instalata separat si e folosita sa dea inspect si debug la container runtime 
    -deci nu e folosita ideal pt a creea containere, deci, spre deosebire de utilitatile Docker sau Nerd Control, este un Debugging Tool
    -putem teoretic sa creeam containere cu utlitatea CRI dar nu e usor, e doar ca sa fie folosita pt special debugging purposes 
    -si merge cumva cu kubelet si stim ca kubelet este responsabil pt a se asigura ca numarul specific de containere sau pod-uri sunt disponibile pe un nod la un moment dat 
    -deci, daca creeam noduri cu utilitatea cri control, kubelet va merge eventual si va sterge aceste containere, deoarece kubelet nu stie de aceste containere sau pod-uri creeate inafara cunostintei lui 
    -utilitatea ctrictl e foarte similara si ea cu docker cli si mai e folosita pt a face chestii cu containerele de ex:
    $ crictl pull busybox   //pt a da pull la imaginea busybox 
    $ crictl images         //pt a lista imaginile pull-uite 
    $ crictl ps -a          //pt a lista containerele 
    $ crictl exec -i -t 3e025dd50a... ls  //pt a executa comanda ls intr un container cu idu 3e0...
    $ crictl logs 3e025dd50a...     //pt a vedea log urile din containerul respectiv 
    
    -o mare diferenta fata de docker este ca crictl stie si de pods, deci putem lista pods urile ruland comanda CRI control 

    $ crictl pods 

    -inainte foloseam docker pt a face lucrurile astea, acuma putem folosi crictl care are aproape aceaasi sintaxa 
    -aici putem vedea tabelul cu toate comenzile crictl vs docker si cele pt pods https://vineetcic.medium.com/mapping-from-dockercli-to-crictl-life-after-docker-is-cri-a39ea5649d6c 
    
    -deci avem ctr command line utility care vine si merge cu ContainerD care e folosita doar pt debugging purposes si are un set limitat de feauture-uri, deci ideal nu vom folosi asta deloc, deci o putem ignora 
    -dupa care avem NerdControl CLI, care e la fel din comunitatea ContainerD, dar este folosit pt purpose uri generale de a creea containere si suporta mai multe feauture-uri, pe care o vom folosi mai mult  
    -apoi avem cri control utility(crictl), care e din comunitatea kubernetes folosit in principiu sa interactioneze cu CRI compatible runtime, deci nu e doar pt ContainerD, poate fi folosit pt toate CRI supported runtimes. In principiu este folosit pt debugging purposes

Pods 

    -presupunem ca aplicatia este deja scrisa si builduita in Docker Images si este disponibila pe un repository Docker ca Docker Hub ca sa poata kubernetes sa ii dea pull 
    -mai presupunem ca clusterul kubernetes a fost deja set up si merge. Poate fi un single-node setup sau un multinode setup, nu conteaza. Toate serviciile trebuie sa ruleze.

    -cum am mai zis, cu Kubernetes, scopul nostru final este sa dam deploy la appu nostru sub forma de containere pe un set de machine-uri care sunt configurate ca noduri worker intr-un cluster 
    -oricum, Kubernetes nu da deploy la containere direct pe nodurile worker
    -containerele sunt incapsulate intr-un obiect Kubernetes numit PODs 
    -un POD este o singura instanta a unei aplicatii 
    -un POD este cel mai mic obiect care poate fi creat in Kubernetes 

    -sa zicem ca avem cel mai simplu caz: un singur node pe clusterul kubernetes cu o singura instanta a aplicatiei noastre care ruleaza intr-un singur container docker incapsulat intr-un singur POD 
    -adica avem pe cluster un singur worker node care are un singur POD cu un singur container incapsulat in el(un POD poate avea mai multe containere, unul pt fiecare serviviu al aplicatiei: backend, frontend, database)
    -acuma, daca numarul de useri care acceseaza aplicatia creste si va trebui sa dam scale up la aplicatia noastra, adica sa adaugam instante aditionale ale aplicatiei noastre web(sa zicem) ca sa se imparta load-ul(incarcatura) 
    -ca sa dam scale up la app, facem alt POD in care este incapsulat alt container, deci o noua instanta a aplicatiei
    -acum, avem doua instante ale aplicatiei noastre web care sunt running pe doua POD-uri separate in acelasi node  

    -sa zicem ca numarul de useri creste mai mult si nodul nostru curent nu are suficienta capacitate pt acesti useri multi 
    -atunci putem creeam un nou node in cluster pe care sa dam deploy la POD-uri aditionale
    -vom avea un nou node adaugat pe cluster ca sa creste, capacitatea fizica a clusterului 
    -deci, POD-urile de obicei, au o relatie de 1 la 1 cu containerele pe care ruleaza aplicatia
    -ca sa dam scale up, creeam noi POD-uri si ca sa dam scale down, stergem POD-uri 
    -NU adaugam containere extra la un POD existent sa scalam aplicatia 
    
    -am zis ca POD-urile au o relatie de 1 to 1 cu containerele, dar nu suntem restrictionati sa avem un singur container intr-un POD 
    -un POD poate avea mai multe containere doar ca nu sunt de obicei containere de acelasi fel  
    -cum am zis si mai devreme, daca vrem sa dam scale up la app trebuie sa creeam alte POD-uri 
    -dar, uneori am putea avea un container ajutator care face un task de support pt appu web(procesarea de date bagate de user, procesarea unui file uploadat de user, etc) si vrem ca acest container helper sa traiasca langa containerul aplicatiei noastre 
    -in acest caz, putem avea ambele containere parte din acelasi POD ca atunci cand un nou container de aplicatie este creeat(python), helperul este creeat de asemenea 
    -cand containerul appului moare, POD-ul moare de asemenea din moment ce sunt parte din acelasi POD 
    -cele doua containere pot comunica unul cu altul direct referindu-se intre ele ca localhost, din moment ce share-uiesc acelasi network space 
    -in puls, pot sa share-uiasca si acelasi spatiu de stocare de asemenea 
    
    -sa presupunem ca vrem sa dam deploy la un app pe un host Docker, vom da efectiv run la container cu comanda  $ docker run python-app  si aplicatia merge ok si userii o pot accesa 
    -cand creste load-ul, dam deploy la mai multe instante ale aplicatiei ruland mai multe comenzi docker run 
    -dupa cv timp aplicatia devine mai complexa si va avea nevoie si de un helper container care paseaza date pt site de altundeva 
    -aceste containere helper mentin o relatie de 1 la 1 cu containerul aplicatiei noastre si deci trebuie sa comunice direct cu containerul aplicatiei si sa acceseze date din acele containere 
    $ docker run helper -link app1, 2, 3...
    -pt asta, trebuie sa facem un map in care sa avem ce containere de aplicatie si helper sunt conectate impreuna, avand mai multe instante ale acestora 
    -v-a trebui sa facem si network conectivity intre aceste containere singuri folosind link uri si network uri custom, va trebui sa creeam si volume share uibile printre containere 
    -ne va trebui o mapa si pt astea si cel mai important va trebui sa si monitorizam state ul containerelor aplciatiei si cand dau fail sa oprim containerul helper aferent containerului aplicatiei pt ca nu mai avem nevoie de el 
    -cand un nou contaioner e deployed va trebui sa dam deploy la un nou container helper de asemenea 
    -cu PODs, kubernetes face toate acestea pt noi automat, noi doar trebuie sa definim ce containere are un POD si containerele din POD vor avea acces by default la acelasi storage, acelasi network namespace si aceeasi soarta pt ca vor fi creeate impreuna si distruse impreuna 
    -chiar daca aplicatia noastra nu ar fi asa complexa si am putea sta doar cu un singur container, kubernetes tot ne cere sa creeam PODs 
    -dar asta e bine pe termen lung, pt ca appu nostru este acuma pregatit pentru schimbari arhitecturale si scalare in viitor 
    -totusi, PODs urile cu mai multe containere sunt un use-case rar si in acest curs ne vom rezuma la PODs cu un singur container 

    -comanda kubectl defapt da deploy la un container Docker, creeand un POD 

    $ kubectl run nginx --image nginx
 
    -deci comanda creeaza automat un POD si apoi da deploy la o instanta de imagine docker nginx 
    -trebuie sa ii spunem de unde ia imaginea aplciatiei. Pt asta, trebuie sa specificam numele imaginii folosind parametrul --image 
    -imaginea app-ului, in acest caz imaginea nginx, este downloadata(pulled) de pe docker hub repository
    -docker hub este un repository public unde ultimele versiuni de imagini docker(latest) ale mai multor aplicatii sunt stocate(nginx, dotnet, postgres, angular, etc..)
    -putem configura kubernetes sa dea pull la imagine de pe docker hubu public sau un repository privat din organizatie
    
    -acum ca am creat un POD, ca sa vedem lista de PODs disponibile avem comanda:

    $ kubectl get pods 

    -care ne ajuta sa vedem o lista de POD-uri din clusterul nostru
    -in cazul de fata, daca dam comanda kubectl get pods chiar dupa ce am dat o pe cea kubectl run nginx --image=nginx, vom vedea ca POD-ul este intr-un state de ContianerCreating si in curand se schimba in stateu Running cand va porni
    -in acest caz, nu am facut serverul web nginx accesibil pt userii externi, il putem accesa intern, dinauntrul nodului si mai tarziu vom vedea cum sa facem servicul accesibil userilor finali 

Minikube Demo 

    -mergem aici https://kubernetes.io/ si dam la tasks si install tools section 
    -trebuie instalat command line toolu kubectl pt a menegiui resursele kubernetes si clusteru nostru dupa ce e setat folosind minikube 
    -daca instalam utilitatea kubectl inainte sa instalam minikube va lasa ca minikube sa configureze utilitatea kubectl sa lucreze cu clusterul cand i-l da 
    -deci, kubectl poate lucra in acelasi timp cu mai multe clustere locale sau remote. Este o mica confuguratie pt el de care se va ocupa automat minikube cand porneste, cand da provide la cluster, DOAR daca avem kubectl deja instalat, deci este important 
    -obiectivul nostru este sa facem un cluster pe local machineu nostru, fie ca e linux, windows sau mac, pt win dam aici https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/ 
    -dam run in cmd la comanda pt a downloada binary fileu:   curl.exe -LO "https://dl.k8s.io/release/v1.32.0/bin/windows/amd64/kubectl.exe"
    -asteptam sa se termine si dupa, optional, putem valida instalarea binary ului dand comanda urm pt a donwloada kubectl checksum file: curl.exe -LO "https://dl.k8s.io/v1.32.0/bin/windows/amd64/kubectl.exe.sha256"
    -dupa asta, dam comanda: CertUtil -hashfile kubectl.exe SHA256
    -ne va da un hash si dupa care dam comanda: type kubectl.exe.sha256
    -comanda asta ne va da si ea un hash si vom compara cele doua hash-uri sa fie identice 
    -va instala aici daca dau cmd din searbar windows H:\kubectl.exe
    -putem afla locatia exact cu comanda: where kubectl 
    -adaugam path-ul complet in environment variables 
    -testam ca versiunea de kubectl sa fie aceeasi ca si cea downloadata cu comanda: kubectl version --client
    -vedem ca o sa nea dea la client version v1.32.0 care este si cea din comanda initiala de curl.exe 


  Minikube 

  Setup

    -daca dam comanda: $ kubectl cluster-info , o sa ne dea eroare pt ca nu am instalat minikube sau un cluster de genu asa ca vom face asta acuma intrand aici pt a instala minikube: https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download
    -inainte sa instalam minikube, ne trebuie un dirver pt virtualizare ca docker engine sau virtualbox
    -intram in widnows powershell cu drepturi de admin(run as administrator) si dam comanda: $ wsl --install 
    -mie imi zice ca wsl e deja instalat si ca trebuie sa instalez o distributie gen ubuntu sau debian folosind optiunea -d urmata de numele distributiei dorite din lista afisata sus in comanda comanda: $ wsl --install -d Ubuntu-20.04
    -am dat urm comenzi:

    $ wsl --update --web-download 
    $ wsk --install --web-dwonload 

    -asta a inceput sa descarce distributia Ubuntu
    -imi cere un user dupa care ma baga direct in Ubuntu 
    -daca ies si vreau sa intru iar, trei sa dau comanda:

    $ wsl -d Ubuntu 

    -acuma suntem pe WSL2 ubuntu si dam urm comenzi sa updatam si upgradam pachetele ce ne vor trebui pt docker engine:
    $ sudo apt update
    $ sudo apt upgrade

    -pt a instala dependintele necesare pt docker engine 

    $ sudo apt install apt-transport-https ca-certificates curl software-properties-common

    -ca sa adaugam docker repository:

    $ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"

    -dupa ca sa instalam docker engine si alte componente necesare: 

    $ sudo apt update
    $ sudo apt install docker-ce docker-ce-cli containerd.io

    -daca da eroare facem asa:

    $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
    $ sudo apt update

    -acum ca avem GPG key adaugata instalam docker engine: 

    $ sudo apt install docker-ce docker-ce-cli containerd.io

    -pt a porni docker service:

    $ sudo systemctl enable docker
    $ sudo systemctl start docker

    -verficam daca docker e instalat si e running ruland comanda: 

    $ docker --version      //pt a verifica instalarea ok

    -si comanda 

    $ sudo systemctl status docker    //pt a vedea ca merge serviciu docker

    -acum ca avem docker instalat si running pe wsl2 ubuntu, vom instala minikube pe windows de aici: https://minikube.sigs.k8s.io/docs/start/?arch=%2Fwindows%2Fx86-64%2Fstable%2F.exe+download
    -dam comanda in cmd sau shell: $ minikube version  , sa ne asiguram ca e instalat corespunzator 
    
    -acum va trebui sa configuram docker in wsl2 ca sa ne asiguram ca minikube de pe windows poate accesa docker din wsl2 
    -in terminalul shell ubuntu wsl2 dam comanda urm pt a afla ipu lu docker daemon:

    Option 1:

    $ docker info | grep "Docker Root Dir"

    ERROR: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.47/info": dial unix /var/run/docker.sock: connect: permission denied errors pretty printing info

    -ne da eroare asta si trebuie sa dam permisiuni userlui nostru cu comanda: 

    $ sudo usermod -aG docker $USER  //in loc de $USER punem david care e userul nostru 

    -dam exit si intram inapoi in ubuntu ca sa se aplice schimbarile de perimisuni si verificam asta dand comanda $ docker ps , care ar trebui sa mearga fara nicio problema 
    -acum dam iar comanda: $ $ docker info | grep "Docker Root Dir"

    Option 2: 
    Use DOCKER_HOST Variable (Advanced)

    Set the DOCKER_HOST environment variable in your Windows terminal to use the Docker daemon running inside WSL 2.
    In PowerShell (Windows), run:

    powershell
    Copy code
    $env:DOCKER_HOST="tcp://localhost:2375"

    -am folosit optiunea 1 
    -acum , dupa ce am instalat minikube cu executabilu downloadat de la pagina oficiala pt instlarea acestuia, dam comanda asta sa verificam instalarea:

    $ minikube version

    -acum dam comenzile de minikube:
    -pt a porni minikube cu docker ca si driver:

    $ minikube start --driver=docker

    https://docs.docker.com/engine/daemon/remote-access/   -pt a configura remote access pt docker  sudo systemctl edit docker.service pt edita docker.service ....

    -pt a afla ipu lu ubuntu dam comanda in ubuntu wls2 : $ ip a  , si ne uitam dupa eth0, dupa care la inet gasim 172.17.34.243 

    -intram pe ubuntu si dam comanzile sa setam docker hostu:

    $ export DOCKER_HOST="tcp://172.17.34.243:2375"

    $ echo $DOCKER_HOST


    -in fine, am instalat docker pe ubuntu din start si merge si in powershell , am setat si hocker hsot env var in powershell sa fie ca si mai sus si e ok
    -acuma dau run la un minikube si nu vede docker, trebuie sa facem dockeru din ubuntu remote, ca si mai inainte: 
        -merem pe siteu asta https://docs.docker.com/engine/daemon/remote-access/ si alegem o varianta ori cu edit docker.service ori cu edit daemon.json 
    -sa zicem ca mergem pe varianta 2 cu daemon.json in care facem in /etc/docker fisierul daemon.json: 
    {
      "hosts": ["unix:///var/run/docker.sock", "tcp://127.0.0.1:2375"]
    }
    
    -acuma trb sa dam restart la serviciu cu comenzile: 

    $ sudo systemctl restart docker
    $ sudo systemctl status docker 

!!!!!!!!!!

    -vedem ca nu merge si mergem pe prima varianta cred 
    -vedem ca nu merge nicicum si isntalam minikube pe ubuntu si gata frt 

    -ne asiguram ca merge dockeru pe ubuntu: $ sudo service docker start
    -instalam minikube pe ubuntu wsl2:

    $ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
    $ sudo install minikube-linux-amd64 /usr/local/bin/minikube

  Minikube DEMO

    -apoi rulam minikube in ubuntu: 

    $ minikube start --driver=docker

    -pt a ne asigura ca minikube merge cum trebuie dam comanda: 

    $ minikube status 

    -instalam kubectl ca am uitat de el 

    # Update the apt package index
    $ sudo apt update

    # Install kubectl using apt
    $ sudo apt install -y kubectl

    -vedem ca nu ne lasa cu comanda asta si venim cu asta mare si tare:

    $ sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

    -cu comanda de sus doar downloadam, acuma trei sa l si instalam asa: 

    $ sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

    -ca sa ne asiguram ca e totu ok, verifricam versiunea asa:

    $ kubectl version --client 

    -acuma verificam daca minikube a configurat kubectl:

    $ minikube kubectl -- get nodes 

    -o sa mearga la fel si cu comanda si o sa ne afiseze acelaasi output ca nodu minikube e running: 

    $ kubectl get nodes 

    -acum sa creeam niste deployment uri, folosind acest cluster minikube cu comanda:

    $ kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0

    -o sa ne zica ca deploymentu hello-minikube a fost creeat 
    -dupa asta, rulam comanda urm ca sa vedem ca deploymentu hello-minikube ruleaza

    $ kubectl get deployments

    -acum o sa dam expose la acest deployment ca un serviciu si pt asta ne folosim de comanda kubectl expose deployment... si ne va scrie ca a fost exposed:

    $ kubectl expose deployment hello-minikube --type=NodePort --port=8080

    -dupa putin timp, vom putea vedea deploymentu cand rulam comanda:

    $ kubectl get services hello-minikube 

    -cea mai usoara cale sa accesam serviciul asta este sa il lasam pe minikube sa dea launch la un web browser pt noi:

    $ minikube service hello-minikube

    -pt ca folosim docker pe linux, nu o sa ne deschide browserul dar o sa ne dea URL-ul serviciului: http://127.0.0.1:36523  ,pe care il copiem si ii dam paste in browser 
    -o sa ne afiseze detaliile despre aplicatie 

    -daca vrem sa stergem serviciu hello-minikube, dam comanda:

    $ kubectl delete services hello-minikube 

    -daca vrem sa punem pe pauza Kubernetes fara sa impactam aplicatiile deployed dam:

    $ minikube pause 

    -daca vrem sa scoatem de pe pauza instanta respectiva dam:

    $ minikube unpause 

    -daca vrem sa schimbam limita default de memorie dam:

    $ minikube config set memory 9001 

    -ca sa vedem catalogul easily Kunbernetes services dam: 

    $ minikube addons list 

    -ca sa creeam un al doilea cluster care ruleaza pe un release mai vechi de Kubernetes:

    $ minikube start -p aged --kubernetes-version=v1.16.1

    -ca sa stergem toate clusterele minikube cu serviciile aferente, dam comanda:

    $ minikube delete --all 


    -mai repet odata doar pt serviciu minikube:

    $ minikube start        // pt a porni clusteru 

    $ kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0      // pt a creea un deployment 

    $ kubectl expose deployment hello-minikube --type=NodePort --port=8080      // pt a expune serviciul pe portu 8080 

    $ kubectl get services hello-minikube       // pt a vedea deploymentu 

    $ minikube service hello-minikube       // pt a vedea linku catre deployment (http://127.0.0.1:35449/)

    * $ kubectl port-forward service/hello-minikube 7080:8080     //pt a face port forwarding care va schimba linku: http://localhost:7080/       daca dam ctrl c in terminalu linux se termina port forwardingu 

    $ kubectl delete services hello-minikube        //pt a sterge serviciul hello-minikube 


  PODs DEMO 

    -dupa cum am zis, un POd este cea mai basic si mic aunitate in Kubernetes 
    -vom folosi utilitate kubectl pt a interactiona cu clusteru Kubernetes 
    -dam comanda kubectl run nginx(numele POD-ului care poate fi orice) --image=nginx (numele imaginii docker dupa repou docker hub sau orice alt container registry)

    $ kubectl run nginx --image=nginx           // daca inainte am dat comanda minikube delete -all trebuie sa dam iar minikube start pt a porni clusteru pt a putea creea POD-ul cu imaginea 

    -odata ce ne zice pod/gninx created, putem sa ii verificam statusu cu coamnda get pods:

    $ kubectl get pods 

    -aici putem vedea coloana cu NAME care este nginx(numele POD-ului creeat adienaori)
    -vedem ca statusu e RUNNING, vedem si coloana READY care arata numarul containerelor de pe POD care sunt in state-ul READY 
    -pe coloana RESTARTS vedem de cate ori a fost restartat sau daca a fost restartat containerul de cand a fost creat 
    -pe coloana AGE vedem de cat timp ruleaza POD-ul 
    -putem vedea mai multe informatii despre POD, ruland comanda:

    $ kubectl describe pod nginx 

    -ne va da foarte multe infomatii despre POD, de ex, numele POD-ului dat de noi(nginx), label-urile care au fost asignate POD-ului: run=nginx - asta a fost creeat by default cand am rulat comanda run 
    -ne arata cand a fost creeat cu exactitate  
    -ne arata NODE-ul caruia ii este asignat acest POD, impreuna cu IP-u nodului: minikube/192.168.42.2 
    -deci in cazu de fata, avem un single node cluster(un culster cu un singur node) setat folosind minikube(cand am dat minikube start - minikube are un singur nod)
    -numele NODE-ului si IPu sunt ale lui minikube

    -apoi, sub Status:  Running, ne arata si IP-ul POD-ului: 10.244.0.3
    
    -mai departe, ne arata informatii despre container, deci stim ca este un singur container, care foloseste imaginea nginx
    -daca erau mai multe containere, le vedeam listate aici aliniate la fel ca nginx sub el sau asa     
    -aici mai vedem si ca imaginea nginx a fost pull-uluia de pe docker hub 
    
    -daca dam scroll pana jos, vom vedea informatii aditioanle numite Events si aici vedem o lista de event-uri care au avut loc de cand a fost creeat POD-ul
    -a trecut prin mai multe Stage-uri de cand a fost pornit
    -vedem ca POD-ul a fost asignat NODE-ului minikube : Successfully assigned default/nginx to minikube 
    -daca erau mai multe NODE-uri, am fi vazut aici carui NODE ar fi fost asignat POD-ul 
    -apoi, a intrat in faza de pulling in care imaginea nginx a fost pulled down de pe docker hub cu succes 
    -apoi, containerul numit nginx a fost creeat si pornit (Created conainer nginx / Started container nginx)

    -mai este inca o comanda pe care o folosim sa aflam statusul POD-ului, aceeasi comanda ca si inainte(kubectl get pods) cu optiunea wide(-o wide):

    $ kubectl get pods -o wide 

    -ca si inainte, vedem numele NODE-ului(minikube) pe care ruleaza POD-ul si IP-ul POD-ului(10.244.0.3), de asemenea
    -asta este IP-ul intern al POD-ului, deci fiecare POD primeste un IP intern propriu in clusterul Kuberenetes 

    -exista o comanda similara si pt NODE-uri:

    $ kubectl get nodes -o wide 

    -aici vedem numele NODE-urilor, in cazu nostru, avem unul singur numit minikube caruia ii este asignat singurul POD numit nginx 
    -mai vedem Statusu acestora, al nostru este Ready, ROLE-ul acestora, al nostru este control-plane 
    -timpu cat a rulat, versinea de minikube, IP-ul intern(192.168.42.2) si extren(daca avem configurat unul)
    -imaginea sistemului de operare - Ubuntu 22.04.4 LTS , versiunea kernel etc..

PODs with YAML

    -Kuberenetes folosiste file uri YAML ca si input-uri pt creearea de obiecte ca si PODs, replici, servicii de deployment...
    -un yaml in kubernetes contine mereu 4 niveluri top level:

    pod-definition.yaml 

    apiVersion: v1 
    kind: Pod 
    metadata:
      name: myapp-pod 
      labels:
        app: myapp 
        type: front-end 
    spec:
      containerrs:
        - name: nginx-container
        image: nginx 


    -acestea sunt proprietatile top level sau root level 
    -ele sunt de asemenea, filed uri obilgatorii, deci trebuie sa le avem neaparat in configuration fileu nostru(pod-definition.yaml)

    -primul este apiVersion, care este versiunea de Kuberenetes API pe care o folosim sa creeam obiectele. Depinzand de ce vrem sa creeam, trebuie sa folosim versiunea corecta de API 
    -acum pt ca lucram cu PODs, vom seta apiVersion cum vrem:   apiVersion: v1 
    -avem asa: pt kind: Pod -> v1, Service -> v1, ReplicaSet -> apps/v1, Deployment -> apps/v1 

    -urm este kind: , care este tipul obiectului pe care vrem sa il creeam, care in cazul nostru este un Pod. Alte valori posibile pot fi: Service, ReplicaSet sau Deployment  

    -urm este metadata: , care se refera la datele despre obiect: name, labels, app...
    -spre deosebire de primele doua proprietati(apiV, kind), unde am specificat doar o valoare string(v1, Pod), aceastra prop este sub forma de Dictionary 
    -deci totul sub metadata este identat catre drepata(are cateva spatii in interior gen) si deci, name and labels sunt childern ai metadata
    -numarul spatiilor de dinaintea proprietatilor names si labels nu conteaza, dar ar trebui sa fie acelasi, din moment ce sunt siblings 
    -daca de ex, labels: avea doua spatii in plus fata de cele doua pe care le are deja, ar fi in interiorul lui name: si deci, ar fi fost un child al proprietatii name: , in loc de sibling, ceea ce ar fi gresit 
    -evident, cele doua proprietati(name si labels) trebuie sa aiba mai multe spatii decat parentu lor, metadata, deci cele doua trebuie sa fie identate catre dreapta unpic(un doua spatii) 
    -daca ar avea toate 3(metadata, name si labels) acelasi numar de spatii inaintea lor(gen ar fi toate aliniate la fel, nu ar fi cele doua prop in interiorul metadata), toate ar fi siblings, ceea ce nu e corect 
    -sub metadata, name-ul este o valoare string, deci ne putem denumi POD-ul myapp-pod 
    -iar labels: este un Dictionary care face parte din Dictionarul metadata si poate avea orice key value pairs pe care ni le dorim 
    -deocamdata, am adaugat un label app cu value-ul myapp. La fel, putem adauga si alte lable uri daca le vedem rostul, care ne vor ajuta sa identificam aceste obiecte mai tarziu 

    -sa zicem ca avem sute de PODs care ruleaza un app frontend si alte sute de PODs care ruleaza un app backend sau o baza de date 
    -o sa ne fie greu sa grupam aceste PODs-uri odata ce sunt deployed. Daca le dam label-uri de acuma(punem fix sub app: type: front-end), gen, frontend, backend, database, vom putea filtra PODs-urile bazate pe aceste label-uri mai tarziu
    -e important de stiut ca sub metadata: putem specifica doar name sau labels sau oricelatceva la care se asteapta Kubernetes sa fie sub metadata. Nu putem adauga orice alta proprietate cum vrem noi sub metadata. 
    -dar, spre deosebire de metadata: , sub labels: putem avea orice fel de key value pairs dupa cum dorim
    -deci, e important sa intelegem la se asteapta fiecare dintre aceste proprietati 
    -pana acuma, am mentionat type-ul si name-ul obiectului pe care dorim sa l creeam, care este un Pod cu numele myapp-pod, dar nu am specificat containerul sau imaginea de care avem nevoie in Pod 
    
    -ultima sectiune in fileu de configurare este sectiunea specification(spec: )
    -depinzand de obiectul pe care vrem sa-l creeam, aici trebuie sa punem informatii aditionale lui Kubernetes despre obiectul respectiv 
    -informatiile acestea vor fi diferite pt obiecte diferite, deci este important sa intelegem sau sa mergem la documentatie sa luam formatu corect pt fiecare obiect(Pod, Service, Deployment..)
    -cum creeam un Pod cu un singur container in el, e usor 
    -spec este un Dictionary, deci adaugam o proprietate sub el numita containers: 
    -containers: este un array, deoarece Pods pot avea mai multe containere in ele 
    -in acest caz, vom adauga un singur item in lista pt ca vrem sa avem un singur container in Pod 
    - liniuta de dinainte de - name: indica ca asta e primu item din lista 
    -itemu din lista este un dictionar, deci adaugam proprietatilen name: si imagine:  
    -value-u pt image: este nginx, care este numele imaginii docker din repositoryu docker hub 

    -Pod este creeat cu comanda:

    $ kubectl create -f pod-definiton.yml 

    -optiunea -f indica file nameu fisierului de configurare(pod-definiton.yml) 
    
    -in concluzie, avem primele 4 proprietati top level apiVersion, kind, metadata si spec 
    -apoi, adaugam value-uri acestora in fucntie de obiectul care va fi creeat 
    -dupa ce creeam Pod-ul, ca sa il vedem, folosim comanda: pt a vedea o lista de Pods uri disponobile, in cazu nostru e doar unu 

    $ kubectl get pods

    -daca vrem sa aflam info detaliate despre un pod anume, folosim comanda:

    $ kubectl describe pod myapp-pod 

    -asta ne va zice cand a fost creeat podu, ce lable-uri are asignate(app=myapp, name=myapp-pod), ce containere docker sunt in Podu respectiv si event-urile asociate cu acest Pod 
    
DEMO: Pods with YAML 

    -in acest demo. vom creea un Pod, din nou, dar de data aceasta, in loc sa ne folosim de comanda kubectl run, vom creea Podu folosin un YAML definition file 
    -scpou este sa creeam un file YAML cu specificatiile Pod-ului in ea 
    -sunt multe modalitati de a o creea, putem face una in orice editor text gen vs code pe windows sau pe linux folosim editor gen vi sau vim sau nano. Un editor cu suport pt YAML este foarte de folos pt anu gresi sintaxa gen vim sau notepad++ 

    -dam numele fisierului pod.yaml cu comanda: 

    $ vim pod.yaml 

    -vom incepe cu cele 4 proprietati root level (apiVersion, kind, metadata, spec)

    pod.yaml 

    apiVersion: v1 
    kind: Pod 
    metadata:
      name: nginx 
      labels:
        app: nginx 
        tier: frontend 
    spec: 
      containers:
      - name: nginx
        image: nginx 

    -stim ca versiunea de API pt un Pod este v1, kind: este Pod(neaparat cu P mare, e case sensitive)

    -metadata: e un dictionar si poate avea values in care definim numele Pod-ului, ca nginx. 
    -putem avea si labele-uri aditionale pe care le putem specifica sub labels: 
    -deci labels: este de asemenea un dictionar, care poate avea cate labele-uri dorim sub el 
    -deci putem specifica un label care e o pereche cheie-valoare(key value pair), de ex, app: nginx si tier: frontend, orice ne poate ajuta sa grupam acest Pod 

    -trebuie sa ne asiguram intre timp ca avem identarea coreceta, de ex, app si tier sunt childerni ai proprietatii label, deci trebuie sa fie pe aceeasi linie veriticala(sa aiba exact acelasi nr de spatii de la capatu randului pana la ele efectiv)
    -si la fel, sub metadata, avem name si labels(identate identic pe aceeasi linie veriticala), care sunt copiii lui metadata: 
    -deci tre sa ne asiguram ca spatierea e corecta. De obicei sunt doua spatii sau un tab, dar nu e recomandat sa folosim tab-uri, deci mereu punem 2 spatii copiilor propietatilor.

    -apoi, trebuie sa definim spec: , care este de asemenea un dictionar si are un object(property) numit containers: 
    -acum, configuram containers: care este o lista de obiecte. Prima oara ii dam un nume, acesta este numele containerului din Pod si pot fi mai multe containere, care fiecare pot avea un nume diferit
    -deci un container poate fi denumit app si altul helper, orice nume face sens pt noi 
    -in cazu de fata se va numi nginx ca si numele imaginii (- name: nginx
                                                               image: nginx )
    -daca folosim alte docker registries fata de oficialu docker hub, trebuie sa specificam pathu full catre acel repository de images in dreptul prop. image: 
    -putem adauga mai multe containere unui Pod, declarand un element secundar la fel ca primul listei(calupul - name, image dedesubt)
    -de ex: - name: busybox
              image: busybox 

    -in cazu nostru, vom folosi un singur container, deci nu vom scrie si exemplu de sus in configuratia noastra 
                                                        
    -acum vom apasa Esc sau ctrl+o urmat de :wq! ca sa dam save la file si dam comanda $ cat pod.yaml sa ne asiguram ca am creat fileu cu contenturile respective si ca am facut spatierea ok 
    
    -acum putem folosi comanda kubectl create sau kubectl apply, deoarece comenzile create si apply merg la fel gen daca creeam un nou obiect, deci putem folosi fie create, fie apply
    -si pasam numele file ului in comanda folosind optiunea -f:

    $ kubectl apply -f pod.yaml     //$ kubectl create -f pod.yaml 

    -care o sa dea eroare si incepem prin a verifica daca e pornit clusterul minikube cu comanda: $ minikube status   , si daca nu e pornit dam $ minikube start 
    -dam din nou comanda kubectl apply dupa ce pornim clusterul minikube si dupa dam comanda: $ kubectl get pods  , sa ne asiguram ca merge Pod ul creat si merge, dar k8s l-a updatat pe cel creeat ieri cu noua configuratie 
    -asa ca, ca sa fiu io sigur ca un nou Pod e creeat de fiecare data, stergem Podul existent si aplicam iarasi configuratia cu kubectl apply:

    $ kubectl delete pod nginx 
    $ kubectl apply -f pod.yaml 

    -si aici vom vedea ca este un Pod creat fresh, nu ca inainte, refolosit si updatat cel de ieri 
    -ca si inainte daca vrem mai multe detalii despre Pod-ul creat, dam comanda:

    $ kubectl describe pod nginx 

    -deci, sa rezumam, prima oara creeam in linux fileu de config, cu comanda vim pod.yaml si il scriem, dupa care, pt a creea Pod-ul, folosim comanda $ kubectl create/apply -f pod.yaml 

Tips and Tricks 

    -instalam vs code cu extensia kubernetes ca sa ne fie mai usor sa scriem fisiere yaml pe care sa nu le gresim 

    containers:
    - name: nginx
      image: nginx

    -aici trebuie sa fim atenti la numele imaginii pt ca nu il valideaza extensia 

    -dar sunt validate anumite filed-uri din configurare, care trebuie sa aiba valori supported by k8s, ca si fieldu kind: care trb sa aiba value-ul Pod 
    -in Explorer, la Outline avem toata structura file ului yaml cu toate proprietatile, dictionarele si array urile pe care le am definit 
    - { } - pt dictionare, [ ] pt array , abc pt proprietati key value 

    $ kubectl apply -f nginx.yaml 

    $ kubectl get pods 
    
    $ kubectl delete pod nginx-2 

Training: 

apiVersion: v1
kind: Pod
metadata:
  name: postgres
  labels:
    tier: db-tier
spec:
  containers:
    - name: postgres
      image: postgres
      env:
        - name: POSTGRES_PASSWORD 
          value: mysecretpassword 

Lab:

    Task: How many nodes are part of the cluster?

    $ kubectl get nodes     => 1 Node si vers de k8s e v1.31.0+k3s1

    Task: What is the flavor and version of Operating System on which the Kubernetes nodes are running?

    $ kubectl get nodes -o wide     // pt a afla versiunea de operating system pec are merg nodurile kubernetes: Alpine Linux v


Lab: Pods with YAML

    Task: How many pods exist on the system?
    
    $ kubectl get pods    // 0

    Task: Create a new pod with the nginx image.

    $ kubectl run nginx --image=nginx     //pt a creea si porni un pod cu imaginea nginx direct din command line / ca sa il creeam dupa un .yaml file folosim kubectl apply/create

    Task: How many pods are created now?

    $ kubectl get pods    // 4 ca a mai facut ei cateva

    Task: What is the image used to create the new pods? You must look at one of the new pods in detail to figure this out.

    $ kubectl describe pod newpods-7smnj    //si mai jos la Containers vedem ca are la Image: busybox

    Task: Which nodes are these pods placed on? You must look at all the pods in detail to figure this out.

    $ kubectl describe pod newpods-7smnj    // si asa punem toate numele de pods si ne uitam la Node la fiecare si avem: controlplane, controplane, controplane, controplane

    Task: How many containers are part of the pod webapp? Note: We just created a new POD. Ignore the state of the POD for now.

    $ kubectl get nodes     // aici il vedem pe cel cu numele webapp caruia ii dam describe:

    $ kubectl describe pod webapp     // aici vedem ca la containers avem mai multe containere doar vazand mai multe Container ID-uri si nume de imagini si nume de containere: nginx agentx

    Task: What is the state of the container agentx in the pod webapp? Wait for it to finish the ContainerCreating state

    $ kubectl describe pod webapp     // tot in comanda asta vedem la Containers: > agentx -> State: Waiting

    Task: Why do you think the container agentx in pod webapp is in error? Try to figure it out from the events section of the pod.

    -ne uitam tot in comanda descrive, la Events: si vedem ca dupa ce a dat pull la imaginea nginx ne da si logu ca a dat fail sa dea pull la imaginea agentx pt ca nu exista numele imaginii sau trb autorizatie si n are
    -rasp a fost ca nu exissta o imagine pe docker hub registry cu numele asta 

    Task: What does the READY column in the output of the kubectl get pods command indicate?

    -coloana READY din comanda kubectl get pods se refera la numarul de containere din fiecare POD care este READY din numarul total de containere al acelui POD 1/2
    -deci raspuns final Running Containers in POD/Total Containers in POD   , de ex: 1/2

    Task: Delete the webapp Pod. Once deleted, wait for the pod to fully terminate.

    $ kubectl delete pod webapp     // pt a sterge podu webapp 

    Task: Create a new pod with the name redis and the image redis123. Use a pod-definition YAML file. And yes the image name is wrong!

    $ nano redis.yaml   // dam comanda asta si scriem configurarea asta 

    redis.yaml 

    apiVersion: v1
    kind: Pod
    metadata: 
      name: redis
      labels:
        app: redis 
    spec:
      containers:
        - name: redis
          image: redis123 

    $ kubectl create -f redis.yaml

    Task: Now change the image on this pod to redis. Once done, the pod should be in a running state.

    -daca dam $ kubectl describe pod redis  , vedem ca la events apare eroare ca e numele imaginii gresit. modificam numele imaginii in .yaml si mai bagam o fisa 

Replication Controllers and ReplicaSets

    -Controllerele sunt creierele de sub capota kubernetes, sunt procesele care monitorizeaza obiectele kubernetes si raspund conform 
    -acum vom vorbi despre replication controller. deci ce e o replica si de ce avem nevoie de replication controllers?
    -ne intoarcem la scenariu unde am aveam un singur POD care rula aplcatia noastra. 
    -daca dintr un motiv anume, PODu da crash si aplicatia noastra da fail? Userii nu vor mai putea accesa aplicatia noastra 
    -ca sa prevenim acest lucru, vrem sa avem mai mult de o instanta sau un POD care ruleaza in acelasi timp 
    -astfel, daca unu da fail, inca avem aplicatia care merge pe celalalt  
    -Replication Controller ne ajuta sa rulam mai multe instante ale unui singur POD in clusteru Kubernetes, oferind astfel high availability
    -chiar daca vrem sa avem un singur pod, putem folosi Replciation Controller 
    -in acest caz, replication controller ne poate ajuta prin a da drumu automat la un nou POD cand cel actual da fail 
    -astfel, replication controller se asigura ca numarul specificat de POD-uri merg in acelasi timp, chiar daca ie 1 sau 100

    -alt motiv pt care avem nevoie de replciation controllers este sa creeam multiple porturi ca sa share-uiasca load-ul pe ele 
    -de ex, in acest caz simplu, avem un singur POD care deserveste un set de useri si cand nr de useri creste, dam deploy la un POD aditional sa echilibram load-ul pe cele doua POD-uri 
    -daca cererea inca creste, si nu mai avem resurse pe primul NODE, putem sa dam deploy la PODs uri aditionale si pe alte NODEs din cluster 
    -replication controller se intinde pe mai multe NODEs din cluster 
    -ne ajuta sa echilibram(balance) load-ul pe mai multe PODs pe diferite NODEs si sa dam si scale la aplicatia noastra cand cererea creste(adica sa dea deploy la alte PODS pe alte NODEs)

    -e important de notat ca exista doi termeni similari: Replication Controller si Replica Set 
    -ambele au acelasi scop, dar nu sunt la fel 
    -Replication Controller este tehnologia mai veche care este inlocuita de Replica Set 
    -Replica Set este noua metoda recomandata sa dam set up la app 
    -oricum, teoria de mai sus ramane valabila pt ambele tehnologii 
    -sunt diferente minore in felul in care merge fiecare si ne vom uita la acestea imd 
    -deci, vom incerca sa ramanem la Replica Sets in toate DEMO-urile si implementarile noastre

  Replication Controller

    -incepem prin a creea un definition file pt replication controllers :
  
    rc-definition.yml 

    apiVersion: v1
    kind: ReplicaitonController 
    metadata:
      name: myapp-rc 
      labels:
        app: myapp
        type: front-end 
    spec:
      template:
        metadata:
          name: myapp-pod 
          labels:
            app: myapp 
            type: front-end 
        spec:
          containers:
            - name: nginx-container
            image: nginx 
      replicas: 3

    -ca orice kubernetes definiton file, avem 4 sectiuni: apiVersion, kind, metadata si spec
    -apiVersion este specific in functie de ce creeam. Replication Controller e suportat in kubernetes API Version v1, deci il punem ca v1 
    -kind: dupa cum stim este ReplicationController
    -sub metadata: adaugam un nume si in numim myapp-ec si vom adauga si cateva lable-uri: app si type si le asignam valori 
    -pana acuma, a fost foarte similar ca si creearea de POD din lectia anterioara 
    -spec:(specification) este cea mai importanta parte din definiton file 
    -pentru orice kubernetes definition file, sectiunea spec: defineste ce e inauntrul obiectului pe care il creeam (la pods aveam containerele)
    -in cazu asta, stim ca replication controller creeaza mai multe instante ale unui POD. dar care POD?
    -facem o sectiune template: sub spec: sa dam un template care sa fie folosit de replication controller sa creeze replici 
    -data trecuta am creeat un pod definition file si am putea refolosi continutul file-ului pod-definition.yml pt a popula sectiunea template: 
    -mutam aproape tot continutul pod definition file-ului pod-definiton.yml(tot inafara de apiVersion: si kind:) in interiorul sectiunii de template(identat la dr cu 2 spatii, pt ca sunt copii ai lui template:) a replcation controller-ului 
    -acum avem in file 2 sectiuni metadata una pt replication controller si una pt pod si doua sectiuni spec, una pt fiecare 
    -avem doua definition file uri combinate impreuna, replication controller fiind parintele si definitia pod ului fiind copilul
    -mai trebuie sa mentionam de cate replici avem nevoie in replication controller. Pt asta, aduagam o alta proprietate, replicas:, in interiorul spec(identata cu 2 spatii mai la dr decat spec, adica aliniat cu template:) si punem nr de replici de care avem nevoie dupa : si spatiu 
    -asadar, template: si repicas: sunt copii directi ai lui spec: , deci sunt siblings si trb sa fie pe aceeasi linie verticala 
    
    -odata ce fileu e gata, dam comanda kubectl create cu optiunea -f si parametru rc-definition.yml 

    $ kubectl create -f rc-definiton.yml 

    -si astfel replication controller e creat, si cand acesta e creat, acesta creeaza prima oara POD-urile, cate trebuie, in cazu asta 3, folosind POd's finishing template 
    
    -ca sa vedem lista de replication controllere create, dam comanda: si vom vedea replication controlleru listat 

    $ kubectl get replicationcontroller

    -putem vedea de asemenea numarul dorit de replici sau POd-uri, numarul curent de replici si cate din ele sunt READY 

    -daca vren sa vedem PODs urile creeate de replication controller, dam comanda: si vom vedea 3 PODs running 

    $ kubectl get pods 

    -vedem ca toate incep cu numele replication controllerului, care este: myapp-rc , indicand ca sunt toate creeate automat de replication controller 

  Replica Set

    replicaset-definition.yml 

    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: myapp-replicaset  
      labels:
        app: myapp
        type: front-end 
    spec:
      template:
        metadata:
          name: myapp-pod 
          labels:
            app: myapp 
            type: front-end 
        spec:
          containerrs:
            - name: nginx-container
            image: nginx 
      replicas: 3
      selector:
        matchLabels:
          type: front-end 

    -Replica Set este foarte similar cu replication controller. ca de obicei avem la inceput apiVersion, kind, metadata, spec 
    -API version este putin diferit, este apps/v1 , ceea ce e diferit fata de ce aveam inainte pt application controller(v1) 
    -daca gresim aici, primim o eroare de genu: no matches for kind=replicaset, pt ca api version specificat nu are support pt Replica Set
    -kind: ReplicaSet 
    -la metadata adaugam name: si labels: app: myapp si type: front-end 
    -seciunea spec: e foarte similara cu cea a replication controllerului, are sectiunea template: unde copiem contentu pod def file incepand cu metadata si dupa seciunea de replicas: 3
    -si acuma e diferenta dintre replication controller si replica set :
    -replica set mai are nevoie si de sectiunea selector:(la nivel cu template si replicas)
    -selector: ajuta replica set sa identifice ce PODs sunt nemegiuite de acesta 
    -dar dc trebuie sa mentionam ce PODs sunt ale replica sets-ului nostru, daca am pus deja continutul definition fileului POD ului in template: ?
    -pentru ca replica sets poate sa menegiuiasca de asemenea PODs-uri care nu au fost creeate ca si parte din creerea replica sets(adica nu au fost creeate cand a fost replica sets creeat)
    -de ex, sunt PODs uri creeate inainte creearii replica setului, care se potrivesc cu label-urile din selector 
    -replica setsu va lua de asemenea in considerare si acele PODs cand creeaza replicile 
    -selector: este una din diferentele majore dintre replication controller si replica sets 
    -Selector: nu este o proprietate required in cazul unui Replication Controller
    -daca nu specificam niciun selector:, acesta va fi ca label urile din pod definition file(din template)
    -dar, in cazu Replica Sets, un input de la user este required pentru aceasta propietate(propietatea selector e required la replica sets) 
    -si trebuie sa fie scrisa sub forma de matchLabels:(o prop interioara selector: - identata 2 spatii la dreapta sub acesta)
    -si, de ex, prop matchLabels: are ca si prop interioara type:front-end
    -deci, selectorul matchLabels da match efectiv la labelu specificat sub el(type: front-end) la labelu POD-ului 
    -selectoru replica setului are de asemenea multe alte optiuni pentru a da match la label-uri care nu erau disponibile intr-un replication controller 
    -ca de obicei, sa creeam un replica set, dam run la comanda kubectl create dand numele definition fileului ca input:

    $ kubectl create -f replicaset-definition.yml 

    -ca sa vedem replica sets urile create dam run la comanda:
    
    $ kubectl get replicaset 

    -si ca sa vedem lista de PODs dam comanda:

    $ kubectl get pods

    -deci, care e treaba cu lable uri si selectoare? de ce dam label la pods uri si obiecte in kubernetes?
    -avem urm scenariu: am dat deploy la 3 instante a aplicatiei noastre frontend ca si 3 PODs 
    -am vrea sa creem un replication controller sau replica set sa ne asiguram ca avem 3 PODs active oricand 
    -acesta este un usecase al replica sets-urilor. Il putem folosim sa monitorizam PODs-uri existente, daca le avem deja creeate cum erau in exemplu acesta
    -in caz ca nu s creeate, replica set le va creea pt noi
    -scopul replica sets-urilor este sa monitorizeze PODs uri si daca unu din acestea da fail sa dea deploy la altele noi 
    -replica setu este defapt un proces care monitorizeaza PODs urile 
    -acuma de unde stie un replica set ce pods uri sa monitorizeze?
    -ar putea fi sute de alte PODs in clusteru nostru care ruleaza diferite aplciatii 
    -aici label urirea pods urilor in timpul creatiei vine in ajutor (metadata: name: myapp-pod labels: tier: front-end)
    -putem sa dam aceste label uri ca si filtru pentru replica setu nostru 
    -sub selector: folosim filtrul matchLabels: tier: front-end si dam acelasi label pe care l am folosit cand am creeat PODs urile 
    -asa, replica set stie ce PODs uri sa monitorizeze 
    -acelasi concept de label uri si selectoare este folosit in multe alte locuri prin kubernetes

    rc-definition.yml 

    apiVersion: v1
    kind: ReplicaitonController 
    metadata:
      name: myapp-rc 
      labels:
        app: myapp
        type: front-end 
    spec:
      template:
        metadata:
          name: myapp-pod 
          labels:
            app: myapp 
            type: front-end 
        spec:
          containerrs:
            - name: nginx-container
            image: nginx 
      replicas: 3
      selector:
        matchLabels:
          type: front-end 

    -la replication controller, in seciunea spec, sunt 3 sub sectiuni si anume template, replicas si selector 
    -avem nev de 3 replici si am updatat selectoru(ca inainte nu era) bazat pe ce am zis inainte la replicas sets 
    -sa zicem ca avem acelasi scenariu ca mai devreme in care avem 3 PODs existente care au fost deja creeate si trb sa creeam un replica set care sa monitorizeze PODsurile sa se asigure ca sunt cel putin 3 care ruleaza in permanenta 
    -cand replciation controller e creeat, nu o sa dea deploy la vreo noua instanta de POD, deoarece 3 instante POD cu matching labels(au labele uri similare cu cel din selector) sunt deja creeate 
    -in acest caz, chiar trb sa dam un template in sectiunea spec: din moment ce nu ne asteptam ca replica set sa creeze un nou POD la deployment?
    -DA, trebuie, pt ca daca un POD da fail in viitor, replica set trb sa creeze unul nou sa mentina numarul dorit de PODs 
    -si ca replica setu sa poata creea un nou POD, definitia din template: este required

    -sa zicem ca am inceput cu 3 replica sets si in viitor ne decidem sa scalam la 6 
    -sunt mai multe moduri de a updata replica setu sa scaleze la 6 replici 
    -primu este sa updatam numarul replicilor in definition file la 6(replicas: 6)
    -apoi rulam comanda urm sa specificam acelasi file folosind parametrul -f si asta va updata replica sets sa aiba 6 replici 

    $ kubectl replace -f replicaset-definition.yml 

    -al doilea mod sa o facem este sa rulam comanda kubectl scale, folosim parametrul --replicas sa ii dam noul numar de replici si specificam acelasi file ca si input cu -f :

    $ kubectl scale --replicas=6 -f replicaset-definition.yml 

    -putem sa ii dam si numele replica setului si in formatul type name(typeu e replicaset si nameu e cel din def file myapp-replicaset)

    $ kubectl scale --replicas=6 replicaset myapp-replicaset 

    -oricum, daca folosim numele fileului ca input nu va rezulta in numarul de replici sa fie updatat automat in file 
    -adica, numarul replicilor din replica set definition file o sa fie inca 3, chiar daca am scalat replica setu sa avem 6 replici folosind comanda kubectl scale si fileu ca input 
    -sunt si optiuni sa scalam automat replica setu bazat pe load 


    -acum sa facem un scrut overview la comenzi iar:

    $ kubectl create -f replicaset-definition.yml     // pt a creea un reploca set sau practic orice alt obiect in kubernetes(poate fi si pod) depinzand de fileu pe care il dam ca input obligatoriu cu optiunea -f 

    $ kubectl get replicaset    // pt a vedea o lista de replica seturi creeate 

    $ kubectl delete replicaset myapp-replicaset      // pt a sterge replica setu cu nameu replica setului din metadata , asta va sterge si PODs urile pe care le are sub control replica setu  

    $ kubectl replace -f replicaset-definition.yml      // avem comanda kubectl replace pt a da replace sau a updata replica setu 
    
    $ kubectl scale --replicas=6 -f replicaset-definition.yml     // pt a scala replica setu din command line fara sa trebuiasca sa modificam fileu 

DEMO - Replica Sets 

    -scriem definition fileu replicaset.yaml ca si primul din lectia trecuta 
    -in sectiunea spec, punem la template ce am scris in nginx.yaml mai putin primele doua randuri evident 
    -trebuie ca neaparat labelu din template pod ului sa fie exact acelasi cu cel de mai sus din selctor: matchLabels:
    -asta le leaga impreuna, ca sa stie replica setu ce pods uri cu labelu respectiv sa mai meneguiasca in caz ca sunt pods uri de genu existente dinaintea creearii replica setului 
    
    -acum ca avem replica setu pregatit, il putem creea cu comanda:

    $ kubectl create -f replicasets.yaml 

    -dupa care, verificam statusu replica setului cu comanda:

    $ kuvectl get replicaset 

    -vedem ca l a creeat deja, numarul de replici de pod uri e 3 
    -putem sa inspectam statusu pods urilor mai departe cu cmd:

    $ kubectl get pods 

    -vedem aici ca avem 3 replici, 3 pods pentru replica setu creeat si toate pods au un nume unic
    -dar observam ca numele pod ului incepe cu numele replica setului: myapp-replicaset-<id unic>
    -deci, dupa numele pod ului ne dam seama daca este un pod standalone(creeat cu comanda de creere de pod cu definition fileu lui yaml) sau daca este un pod creeat de un replica set
    -am zis ca replica setu se asigura ca un numar suficient(cel specificat in replcias:) de pods sau replicas sunt running mereu 
    -daca dam comanda sa stergem un pod: 

    $ kubectl delete pod myapp-replicaset-dd4g4 

    -acuma daca verificam din nou pods urile cu comanda get pods, vedem ca replica sets are inca 3 pod uri running si veedem ca unu din pod uri a fost creeat mai recent ca celelalte doua 
    -si mai vedem ca numele vechiului pod dd4g4 a fost sters si a fost inlocuit cu un nou pod cu un nume diferit 
    -deci acesta este replica set asigurandu-se ca un numar suficient de pods sunt mereu disponibile pe cluster 
    
    -acum daca rulam comanda describe pt replica sets si vom vedea ca numarul dorit de replici este 3 si aici vedem mai multe informatii despre replica set, exact ca in cazul comenzii kubectl desripe pod command

    $ kubectl describe replicaset myapp-replicaset 

    -aici vedem numele replica setului, selectors(app: myapp), labelu replica setului insusi
    -daca dam scroll mai jos, vedem labelurile de care se foloseste selectorul care sunt label urile pod ului (Pod Template: Lables: app=myapp) si mai vedem si definitia containerului nginx 
    -si mai jos vedem eventurile, deci cand noi am creeat replica setu a creeat si 3 replici, dar apoi am sters una si el a facut alta ca sa mentina nr dorit de replici
    -comanda e f buna pt a inspecta ce s a intamplat cu replica setu, in cazu in care facem troubleshoot la ceva sau cautam mai multe informatii depsre ce s a intamplat cu replica setu , etc 
    -deci am zis ca reploca setu se asigura ca un numar minim de replici sunt disponibile in permanenta 

    -dar ce se intampla daca sunt mai multe replici decat nr required gen 4 in cazu nostru fata de 3 dorite ?
    -sa incercam sa facem un pod care are acelasi label pe care il foloseste replica setu sa creeze noi pods sau sa le aleaga sa le memegiuiasca pe cele deja existente(labels:  app: myapp) 
    -ca sa facem asta, ne intoarcem la definition fileu podului nginx.yaml unde vedem ca e templateu pt a creea un pod cu numele nginx-2 dar i am schimbat si labelu sa fie la fel ca si labelu din templateu de pod din replica set (app: myapp)
    -acuma, sa creeam PODu direct si nu prin replica set, ci direc PODu cum am facut inainte de replica sets. Si sa vedem ce se intampla cand creeam un nou pod inafara replica setului, dar care are aceleasi label-uri pe care le foloseste replica set selectoru(app: myapp)
    -inainte sa facem asta dam comanda pt get pods si vedem ca is doar cele 3 pods care au fost creeate de repilca set si acuma folosim comanda urm si vedem ca podu a fost creeat 
    
    $ kubectl create -f nginx.yaml 

    -dar daca rulam rapid comanda get pods, vedem ca statusu acestui pod este deja intr un state terminating 
    -deci replcia setu ii da terminate la podu nou pe care tocmai l am creeat 
    -nu permite sa existe mai multe pods cu aceleasi lable uri decat nr de replici configurate in replica set(3)
    -deci, daca rulam comanda $ kubectl describe replicaset myapp-replicaset, vom vedea ca la finalul output ului comenzii, la ssectiunea Events, controlleru replica set sterge noul pod nginx-2 pe care tocmai l am creeat: Normal  SuccessfulDelete  3m32s (x2 over 3m43s)  replicaset-controller  Deleted pod: nginx-2
    -acuma daca dam iar comanda $ kubectl get pods , vedem ca a si disparut din linsta(inainte era, dar era cu statusu termianting) 
    
    -acu sa zicem ca vrem sa schimbam numarul replica sets urilor la 4 in loc de 3 cat e acuma. sa zicem ca vrem sa dam scale up la aplicatie 
    -pt asta trei sa editam replica set definition file si sa updatam replicas count la 4 
    -pt asta vom folosi o noua comanda: 

    $ kubectl edit replicaset myapp-replicaset 

    -cand rulam comanda asta, vedem ca deschide running configurationu al replica setului intr un editor text intr un format text 
    -in cazu asta in deschide in vim 
    -sa notam ca asta nu e fileu proproiu zis pe care l am scris si folosit la inceput. Acesta e un temporary file, care e creeat de kubernetes in memorie, ca sa ne lase sa editam configuratia unui existing object in kubernetes 
    -si de asta, vom vedea field uri aditionale in acest file pe langa cele pe care le am scris noi 
    -deci, schimbarile facute acestui file sunt aplicate direct pe running configurarionu de pe cluster imd ce fileu e salvat 
    -deci trei sa fim foarte atenti la schimbarile pe care le facem aici 
    -daca cautam sectiunea specs: unde putem vedea running configurationu si numarul curent de replici care este setat la 3 
    -putem sa il schimbam sa dea scale up la inca un pod, din 3 in 4 , apoi dam save si exit din editor 
    -este un editor tampit asa ca, ca sa putem schimba un 3 in 4 la replicas: trebuie sa ne miscam cu sageti pana acolo, apoi sa apasam pe insert sa putem edita, apoi sa con ctrl c apoi sa scriem acolo :w pt a salva, apoi sa dam iar ctrl c si sa scriem !qa: ca sa dam quit odata 
    -acuma daca dam iar comanda $ kubectl get pods , vedem ca a aparut un nou pod care a fost spawn-at acu cateva secunde 
    -putem folosi acelasi approach sa dam scale down de asemenea 
    -si este si o comanda disponibila sa dea scale la nr de replici fara sa mai trebuiasca sa intram in modu edit al definiton fileului 
    -si acela este sa folosim comanda urm in care ii dam numele replica setului(myapp-replicaset) si ii setam nr de replici carora sa le dea scale la 2 cu --replicas. deci putem specifica un nr mai mare sau mai mic decat nr curent de replici 
    
    $ kubectl scale replicaset myapp-replicaset --replicas=2 

    -acuma daca dam run la comanda get pods iar, vedem ca replica setu da scale down acuma la 2 replici dand terminate 2 din pod uri si vom avea doar 2 pod uri 
    
Coding exercise Replica Sets 

    replicaset-definition.yml 

    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: frontend
      labels:
        app: mywebsite
        tier: frontend
    spec:
      replicas: 4
      template:
        metadata:
          name: myapp-pod
          labels:
            app: myapp
        spec:
          containers:
            - name: nginx
              image: nginx
      selector:
        matchLabels:
          app: myapp

Lab: Replica Sets 

    Task: How many PODs exist on the system? 

    $ kubectl get pods  // 0 

    Task: How many ReplicaSets exist on the system?

    $ kubectl get replicaset  // 0 

    Task: How many pods are desired in replica set? 

    - vedem tot la get replicaset  sau descirbe replicaset  si e 4

    Task: How many ReplicaSets exist on the system?

    $ kubectl describe replicaset new-replica-set     //busibox777 

    Task: How many ReplicaSets exist on the system? 

    $ kubectl get rs  // 0 

    task: de ce crdem ca nu exista niciun pod READY?

    -ca imaginea busybox777 nu exista 

    task: Delete any one of the 4 PODs.

    $ kubectl delete pod new-replica-set-8ts85 

    task: How many pods exist now?

    $ kubectl get pods    // tot 4 pt ca replica setu mentine nr desired de pods 

    task: Why are there still 4 PODs, even after you deleted one?

    - replicaset ensures that desired number of pods always run 

    task Create a ReplicaSet using the replicaset-definition-1.yaml file located at /root/.  There is an issue with the file, so try to fix it.

    -dam nano replicaset-definition-1.yaml
    -la apiversion punem apps/v1 in loc de v1 si dupa dam run la comanda:

    $ kubectl create -f /root/replicaset-definition-1.yaml 

    Task: Fix the issue in the replicaset-definition-2.yaml file and create a ReplicaSet using it.
    This file is located at /root/.

    - la template la labels avem alt tier decat frontend cum avem la selector asa ca il punem frontend in loc de nginx si dam create la replica set 

    $ kubectl create -f /root/replicaset-definition-2.yaml 

    Task: Delete the two newly created ReplicaSets - replicaset-1 and replicaset-2

    $ kubectl delete replicaset replicaset-1 

    $ kubectl delete -f replicaset-definition-2.yaml    // putem setrge si dupa numele replicasetului efectiv si dupoa numele definition fileului cu optiunea -f <numele fileului> 

    Task: Fix the original replica set new-replica-set to use the correct busybox image.
          Either delete and recreate the ReplicaSet or Update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created.

    $ kubectl edit replicaset new-replica-set     //ca sa modificam numele imaginii folosite de templateu de containere dupa care stergem pods urile ca sa creeze altele replicasetu cu noua iamgine 

    -stergem 777 din coada busybox de ka image din spec: containers:  dupa care dam ctrl c si scriem :w sa dam save la modificare si dupa :qa! sa iesim din editoru vietii 
    
    $ kubectl get pods    // ca sa listam si sa stergem pods urile 

    $ kubectl delete pod new-replica-set-dc9c5  //si asa le stergem pe cele 4 initiale care nu is running si noile creeate o sa fie running 

    Task: Scale the ReplicaSet to 5 PODs. Use kubectl scale command or edit the replicaset using kubectl edit replicaset.

    $ kubectl edit replicaset new-replica-set     // dam tot asa edit si save SAU

    $ kubectl scale rs new-replica-set --replicas=5   //scalam direct replciasetu

    Task: Now scale the ReplicaSet down to 2 PODs. Use the kubectl scale command or edit the replicaset using kubectl edit replicaset.

    $ kubectl scale rs new-replica-set --replicas=2 


Deployments 

    -de ex, avem un server web care trei sa fie deployed intr un environment de productie si avem nev de mai multe instante ale web serverului 
    -cand apar versiuni mai noi ale builduruilor aplicatiei pe docker registry vom vrea sa updatam instantele docker ale aplicatiei 
    -dar, cand upgradam instantele noastre, nu vrem sa le upgradam toate deodata, cum am facut inainte pt ca asta va impacta userii care acceseaza aplicatia, deci am vrea sa upgradam instantele una cate una 
    -acest fel de upgrade uri sunt stiute ca rolling upgrades 
    -sa zicem ca unul dintre upgrade urile pe care le am facut a rezultat intr-o eroare neasteptata si ne este cerut sa facem undo la schimbarea asta, deci ne am placea sa putem sa dam roll back la schimbarile care au fost facute recent 
    -in cele din urma, sa zicem ca am vrea sa facem mai multe schimbari la environmentu nostru, ca si upgradarea versiunii de baza a web serverului sau sa scalam environmentul sau sa modificam alocarile de resurse, etc 
    -nu am vrea sa aplicam fiecare change imediat dupa ce e rulata comanda, si in loc am vrea sa aplicam o pauza environmentului, sa facem schimbarile si apoi sa dam resume ca toate schimbarile sa se faca deodata 
    -toate aceste capabilitati sunt disponibile cu kubernetes deployments 
    -pana acum in curs, am vorbit de PODs care dau deploy la care o instanta a aplicatiei noastre ca aplicatia web in cazu asta 
    -fiecare container docker este incapsulat in PODs uri 
    -mai multe astfel de PODS uri sunt deployed folosind replication controllers sau replica sets  
    -apoi vine deploymentu, care este un obiect kubernetes care vine mai sus in ierarhie(replica setu face parte din acesta, e incapsulata in el) 
    -deploymentu ne ofera capabilitatea sa upgradam instantele de baza fara intreruperi, folosind rolling upgrades(upgrade uri din mers gen), sa dam undo la schimbari, sa dam pause si resume la schimbari daca e nevoie 

    -ca sa creem un deployment, ca si inainte, creeam mai intai un deplyment definition file 
    -contentu deployment definition fileului sunt exact la fel ca si cele ale replicas seturilor, doar kind: difera, acum va fi Deployment
    -api version este apps/v1 ca la replica sets, metadata care are name: si labels: 
    -si spec: , care are template: , replicas: si selector: 
    -templateu are in el un pod definition 
    -odata ce e fileu ready, dam run la comanda: 

    deployment-definition.yml 

    
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: myapp-deployment 
      labels:
        app: myapp 
        type: front-end 
    spec:
      selector:
        matchLabels:
          type: front-end   
      replicas: 3
      template:
        metadata: 
          name: myapp-pod
          labels:
            app: myapp   
            type: front-end
        spec:
          containers:
          - name: nginx-container 
            image: nginx 

    $ kubectl create -f deplyment-definition.yaml  

    -apoi rulam comanda get deployments sa vedem deploymentu nou creat

    $ kubectl get deployments 

    -deploymentu creeaza automat un replica set, deci daca rulam comanda get replicaset vom vedea un nou replica set creeat de catre deployment, cu numele deploymentului urmat de un id 

    $ kubectl get replicaset 

    -replica setu creeza in cele din urma pods, deci daca rulam comanda get pods, vom putea vedea pods urile cu numele replica setului si un id 
    -pana acu nu e mare diferenta intre replica seturi si deploymenturi exceptand faptul ca deploymentu a creeat un nou obiect kubernetes numit deployments 
    -vom vedea cum sa profitam de depoyment folosind use case urile pe care le am discutat adineaori in urm lectii 

    -ca sa vedem toate obiectele creeate deodata rulam comanda:

    - kubectl get all 

    -in cazu asta vedem toate ierarhia: vedem ca a fost creeat mai intai deploymentu care apoi creeaza replica set care creeaza dupa aia 3 pods 

DEMO - Deployments 

    -incepem prin a scrie deployment.yaml care se scrie foarte similar cu deployment.yaml 
    -singura diferenta e ca la kind: punem Deployment 
    -creeam fisieru deployments si fisieru definition in acesta
    -creeam deploymentu folosind comanda:

    $ kubectl create -f deployment.yaml 

    -acuma dupa ce s-a creeat rulam comanda pt a vedea deploymentu:

    $ kubectl get deployments 

    -vedem ca avem 3 din 3 pods ready si functionale 
    -daca vrem sa verificam si pods dam get pods si vedem ca sunt 3 pods intr un running state si au numele deploymentu plus idu unic  
    -acu rulam comanda pt infomratii detaliate despre deployment:

    $ kubectl describe deployment myapp-deployment 

    -vedem tot asa informatii despre deplymentu nostru, numele lui, label-urile lui app si tier, selectoru pe care l folosim acelasi ca si al replica setului(app: myapp) 
    -vedem ca avem 3 pods uri desired si 3 running available
    -la sectiunea de pods vedem ca avem labelu myapp, detalii depsre container cum ar fi ca imaginea e nginx 
    -la events vedem mesaju de la deployment controller care zice ca replicasetu pt acest deployment a fost scalat la 3 

    -rulam si comanda  get all care arata toate obiectele create din cluster 

    $ kubectl get all

    -si vedem ca avem un deployment, cel creeat de noi, si un replica set care e creeat pt acel deployment specific si sus avem cele 3 pods uri 

Coding Exercise:

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
      labels:
        app: mywebsite
        tier: frontend
    spec:
      replicas: 4
      template:
        metadata:
          name: myapp-pod
          labels:
            app: myapp
        spec:
          containers:
            - name: nginx
              image: nginx
      selector:
        matchLabels:
          app: myapp

Lab Deployments:

    Task: How many PODs exist on the system? In the current(default) namespace.

    $ kubectl get pods    // 0 

    Task: How many ReplicaSets exist on the system? In the current(default) namespace.

    $ kubectl get replicaset    // 0 

    Task: How many Deployments exist on the system? In the current(default) namespace.

    $ kubectl get deployment      // 0

    Task: How many Deployments exist on the system now? We just created a Deployment! Check again!

    $ kubectl get deployment      // 1

    Task: How many ReplicaSets exist on the system now?

    $ kubectl get replicaset    // 1

    Task: How many PODs exist on the system now?

    $ kubectl get pods    // 4

    Task: Out of all the existing PODs, how many are ready?

    $ kubectl get pods    // 0 sunt ready

    Task: What is the image used to create the pods in the new deployment?

    $ kubectl describe pods    // ne uitam sub Containers: si vedem Image: busybox888

    Task: Why do you think the deployment is not ready?

    - The image busybox888 doesn't exist 

    Task: Create a new Deployment using the deployment-definition-1.yaml file located at /root/. There is an issue with the file, so try to fix it.

    $ nano deployment-definition-1.yaml

    -la kind trb scris deplyment cu D mare si la image trb scris busybox fara 888 
    -asa trb sa arate:

    deployment-definition-1.yaml
    
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: deployment-1
    spec:
      replicas: 2
      selector:
        matchLabels:
          name: busybox-pod
      template:
        metadata:
          labels:
            name: busybox-pod
        spec:
          containers:
          - name: busybox-container
            image: busybox   
            command:
            - sh
            - "-c"
            - echo Hello Kubernetes! && sleep 3600

    -acuma dam deploy la deployment:

    $ kubectl create -f deployment-definition-1.yaml 


    Task: Create a new Deployment with the below attributes using your own deployment definition file.

    Name: httpd-frontend;
    Replicas: 3;
    Image: httpd:2.4-alpine

    -dam nano deployment-definiton-2.yaml 

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: httpd-frontend
      labels:
       tier: httpd-frontend
    spec:
      replicas: 3
      template:
        metadata:
          labels:
            name: httpd-frontend
        spec:
          containers:
          - name: alpine-container
            image: httpd:2.4-alpine
            command:
            - sh
            - "-c"
            - echo Hello Kubernetes! && sleep 3600
        selector:
          matchLabels:
            name: httpd-frontend

    -dupa ce am facut asta, ceeam deploymentu:

    $ kubectl create -f deployment-definition-2.yaml 

    -saau putem fara sa mai creeam definitoon fileu, direct din command line:

    $ kubectl create deplyoment httpd-frontend --image=httpd:2.4-alpine --replicas=3


Deployments - Update and Rollback 

    -cand creeam prima oara un deployment, acesta da trigger la un rollout si un nou rollout creeaza o noua revizie de deployment, sa zicem Revision 1 
    -in viitor, cand versiunea de container e upgradata la una noua(din nginx 1.7.0 la 1.7.1), un nou rollout e triggeruit si o noua versiune de deployment e creeata, numita Revision 2 
    -asta ne ajuta sa keep track la schimbarile facute la deploymentu nostru si ne permite sa dam roll back la o versiune precedenta de deployment daca e necesar 
    -putem vedea statusu rollout-ului daca rulam comanda:

    $ kubectl rollout status deployment/myapp-deployment    //updateaza replicile dupa care ne printeaza successfully rolled out daca e totu ok 

    -ca sa vedem reviziile si istoricu rollout-ului dam comanda: 

    $ kubectl rollout history deployment 

    -sunt doua tipuri de strategii de deployment:
    -sa zicem ca de ex avem 5 replici deployed ale instantei web app-ului nostru 

    -un mod de a le upgrada(Recreate) pe acestea la o versiune mai noua este sa le dam destroy la toate si sa creeam apoi versiuni mai noi ale instantelor aplicatiei 
    -adica prima oara sa dam destroy la cele 5 instante running(nginx 1.7.0) si apoi sa dam deploy la 5 instante noi ale noii versiuni de aplicatie(nginx 1.7.1)
    -problema cu modu asta este ca in timpu in care versiunile mai vechi sunt down si inainte ca versiunile noi sa fie up, aplicatia e jos si inaccesibila pt useri 
    -aceasta strategie e stiuta ca strategia de Recreate si din fericire nu e strategia default de deployment 

    -a doua strategie(Rlolling Update) este sa nu le dam destroy la toate instantele cu versiune veche deodata 
    -asa ca, dam jos o instanta cu versiune mai veche si dam deploy up la una mai noua rand pe rand si facem asta pt fiecare instanta cu versiune veche una cate una
    -adica nu le dam pe toate jos deodata si dupa le dam deploy la cele noi, ci dam una veche jos si dam deploy la una noua si tot asa pt toate instantele vechi 
    -astfel, aplicatia nu cade niciodata si upgradeu e la fel 

    -daca nu specificam strategia in timp ce creeam deploymentu, este rolling update by default 
    
    -cand vorbim de updatarea aplicatiei pot fi upgrade-uri la mai multe caracteristici ale aplicatiei: 
        -updatarea versiunii aplicatiei, facand update la versiunea de container Docker folosit 
        -updatarea label urilor 
        -updatarea nr de replici ...

    -din moment ce avem deja un deployment definition file, e usor pt noi sa modificam acest file:
    -odata ce facem schimbarile necesare, dam comanda urm pt a aplica schimbarile:

    $ kubectl apply -f deployment-definition.yaml 

    -un nou rollout e triggeruit si o noua revizie a deploymentului e creeata 

    -mai este o cale sa facem acelasi lucru: 
    -folosim comanda set image pt a updata imaginea plicatiei noastre, dar facand updateu asa, deploymentu definition fileu va avea o alta configurare(versiunea de imagine veche, fata de actuala versiune a imaginii containerului dupa ce dam comanda de modificare a imaginii):

    $ kubectl set image deployment/myapp-deployment \ nginx-container=nginx:1.9.1

    -deci trebuie sa fim atenti cand folosim acelasi definition file sa facem schimbari in viitor

    -diferenta dintre strategiile Recreate si Rolling Update poate fi vazuta si cand dam kubectl describe la deploymenturi:

    -vom observa ca atunci cand a fost folosita strategia Recreate(o vedem si la StrategyType: ), eventurile indica ca replica setu vechi a fost scalat down la 0 prima oara  
    -si apoi noul replica set este scalat up la 5 

    -cand am folosit strategia Rolling Update, vechiul replica set a fost scalat down cu cate o instanta pe rand si simultan, scalam up noul replica set unul cate unul 
    -adica, de ex, avem replica setu vechi cu 5 instante, ii dam scale daown la 4 si dupa dam scale up la cel nou la 1 si dupa pasu asta dam scale down la vechiul rs la 3 si la cel nou dam scale up la 2 si tot asa...
    
    -sa vedem cum un deployment face un upgrade sub capota:
    -cand un nou deployment este creeat, sa zicem, sa dea deploy la 5 replici, prima oara creeaza un replica set automat(Replica Set -1), care creeaza la randu lui numarul de PODs required sa indeplineasca numarul de replici 
    -cand dam upgrade la aplicatie, cum am vazut in slideu precedent, obiectul kubernetes deployment creeaza un nou replica set sub capota si incepe sa dea deploy la containere acolo 
    -in acelasi timp, da jos pods urile in vechiul replica set, urmand o strategie rolling update 
    -asta poate fi vazut cand incercam sa listam replica seturile folosit comanda get replicasets 
    -aici vedem vechiu replica set cu 0 pods si noul replica set cu 5 pods
    
    -sa zicem ca de ex, dupa ce dam update la aplicatie, ceva nu e chiar ok. Ceva e gresit ce noua versiune de build pe care o folosim sa dam upgrade 
    -deci am vrea sa dam rollback la update 
    -kubernetes deployments ne lasa sa dam roll back la o versiune precedenta 
    -sa dam undo la o schimbare, rulam comanda rollout undo urmata de numele deploymentului 
    -deploymentu va distruge pods urile din noul replica set si le va porni pe cele vechi din vechiul replica set si aplicatia va fi inapoi la vechiul format:

    $ kubectl rollout undo deployment/myapp-deployment 

    -cand comparam outputurile comenzii $ kubectl get replicasets inainte si dupa rollback, vom putea observa diferenta asta 
    -inainte de rollback, primul replica set(cel vechi, adica cel initial) avea 0 pods si noul replica set avea 5 pods 
    -asta va fi inversat dupa ce rollbacku e terminat(cel vechi va avea 5 iar cel nou 0 pt ca am dat rollbacks)

    -comenzile pe scurt: 

    $ kubectl create -f deployment-definition.yml   // pt a creea deploymentu 

    $ kubectl get deployments                       // pt a lista deploymenturile 

    $ kubectl apply -f deployment-definition.yml    // pt a updata deploymenturile 

    $ kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 // tot pt a updata deploymentu si anume imaginea unuia / putem sa schimbam imaginea si cu edit sa fie schimbata si in def file 

    $ kubectl rollout status deployment/myapp-deployment  // pt a vedea statusu rollout urilor 

    $ kubectl rollout history deployment/myapp-deployment  // pt a vedea istoricu rollout urilor si reviziile 

    $ kubectl rollout undo deployment/myapp-deployment    // pt a da rollback la o operatiune de deploy 

DEMO - Deployments - Update and Rollback 

    -folosim definition fileu deployment.yaml si ca sa creeam deploymentu folosim comanda kubectl create -f ..
    -mai intai dam get pods sa ne asiguram ca nu s obiecte creeate in cluster 
    -si daca nu apare nimic, e ok si dam comanda:

    $ kubectl create -f deployment.yaml 

    -folosim o comanda noua numita rollout status command cu numele deploymentului pt a vedea statusu deploymentului 

    $ kubectl rollout status deployment.apps/myapp-deployment 

    -dupa ce rulam comanda, vedem ca deploymentu myapp-deployment a fost rolled out cu succes 
    -daca am fi rulat comanda asta mai repede, gen chiar dupa ce am rulat comanda sa creeam deploymentu, am fi vazut alt status 
    -sa stergem rapid deploymentu cu comanda: 

    $ kubectl delete deployment myapp-deployment 

    -acuma dam iara create la deloyment si rulam rapid comanda rollout status 
    -acum vom vedea statusul fiecarei replici creeate
    -deci vedem ca deploymentu incearca sa dea deploy up la PODs uri unu cate unu: pe primu rand avem 0 of 6 si dupa 1 of 6 
    -deci pana nu sunt toate PODs up and running nu vedem mesaju deployment "myapp-deployment" successfully rolled out 
    -odata deployed, folosim alta comanda sa vedem istoricu(historyu) deploymentului si pt asta vom folosi aceeasi comanda ca si inainte, dar in loc de status vom folosi comanda rollout history 

    $ kubectl rollout history deployment.apps/myapp-deployment 

    -acum o sa vedem ca aplicatia myapp deployment are o singura revizie care e aceea pe care tocmai am facut-o dand deploy la deployment efectiv 
    -pe langa coloana REVISION, mai avemuna CHANGE-CAUSE la care vom observa ca nu este niciun change cause specificata 
    -deoarece nu i am cerut specific sa tina inregistrarea cauzei schimbarii in timp ce am creeat deploymentu, asa ca mergem inapoi si reparam asta, o sa stergem deploymentu iarasi cu delete 
    -asteptam sa termine de creeat pods urile, verificam cu kubectl get pods pana cand dispar toate 
    -dupa ce toate au fost sterse, rulam aceeasi comanda ca si inainte cu create -f ... dar de data asta folosim optiunea --record care ii zice lu kubernetes sa dea record la cazua schimbarii 

    $ kubectl create -f deployment.yaml --record 

    -si acuma rulam comanda rollout status sa vedem statusu deploymentului si asteptam ca toate pods sa fie up si deploymentu sa fie successful 
    -acuma rulam iar comanda hisotry si vedem ca in deptul reviziei 1 este un change cause care e mentionat, care e efectiv aceeasi comanda pe care am rulat o sa creeam deploymentu 
    -dar pt ca am folosit comanda --record va inregistra comanda la CHANGE-CAUSE 
    -acuma sa facem o schimbare deploymentului 
    -prima oara rulam comanda describe cu numele deploymentului:

    $ kubectl describe deployment myapp-deployment 

    -vedem in sectiunea Annotations: a inregistrat comanda pe care am folosit-o sa creeam acest deployment si stim ca ruleaza 6 replici si daca dam scroll down putin, vedem ca imaginea containerelor din deployment este nginx si vrem sa schimbam asta 

    $ kubectl edit deployment.apps/myapp-deployment --record     // pt a edita def file si a inregistra schimbarea in revision history nu trb sa zicem si ce fel de resursa e ca in tutorial 

    -acuma cautam la spec: la containers: la - image: , avem nginx pe care o sa o schimbam intr-o versiune mai veche
    -acuma foloseste cea mai recenta versiune de nginx si noi o vom schimba la o versiune mai lower si pt asta folosim documentatia docker hub 
    -intram pe docker hub si cautam imaginea nginx si vedem ca versiunea curent din mom ce nu am specificat o este 1.19 si hai sa folosim o versiune mai veche ca si 1.18 
    -ca sa schimbam versiunea la cea veche, dupa nginx adaugam : si mentionam versiunea asa: image: nginx:1.18 
    -trebuie sa intram cu permisiuni in modu edit sa ne si lasa sa editam dupa care dam ctrl c si scriem :x sau :w sa saleveze
    -daca dam rollout status vedem ca sterge vechile pods si face altele cu iamginea nginx:1.18 
    -daca dam $ kubectl describe deployment myapp-deployment , vedem ca face acelasi lucru ca mai sus, da scale up la noul replica set la 1 instance si scale down la vechea la 2 instances 
    -sus la Annotations: putem vedea ca e alta revizie: revision: 2, si cauza schimbarii care este comanda pe care am folosit-o sa editam imaginea si anume kubectl edit deployment.apps.....
    -si mai jos la Contaienrs: ne uitam la Containers: nginx: si la Image: vedem ca e nginx:1.18 

    -sa mai facem o schimbare la deployment, acuma foloseste nginx:1.18, sa zicem ca avem requirement sa folosim o imagine diferita in loc de asta 
    -o modalitate de a face asta e sa folosim comanda: $ kubectl edit deployment si sa editam numele imaginii in fisieru YAML care se deschide 
    -o alta modalitate ar fi sa editam noi fisieru manual cu nano, vim sau asa si dupa sa dam comanda kubectl apply -f deployment.yml 
    -o alta cale ar fi sa folosim comanda kubectl set image deployment si numele depolymentului si urmat de numele containerului(nginx), urmat de = si urmat de numele imaginii dupa docker hubs si optiunea --record sa se inregistreze schimbarea in rollout history:

    $ kubectl set image deployment myapp-deployment nginx=nginx:1.18-perl --record 

    -daca rulam acuma rapid comanda status, vedem ca replicile vechi sunt distruse si se fac replici noi cu noua imagine. Acelasi lucru il vedem si la events in comanda describe deployment 
    -daca dam comanda history ne arata ca totu este gata si avem noua versiune de revizie nr 3, unde updatam imaginea folosind comanda de mai devreme set image...
    -rulam comanda get pods sa ne asiguram ca toate pods urile sunt up and running, asta puteam sa vedem si in describe 
    -sa zicem ca este o problema cu noua imagine folosita si nu merge ok pt useri
    -putem da revert back la versiunea precedenta de revizie cu comanda undo. Acuma suntem la revizia 3 si vrem sa dam rollback la revizia 2 care are nginx:1.18 
    
    $ kubectl rollout undo deployment myapp-deployment 

    -acuma rulam rapid kubectl status si vedem ca face schimbarea si da jos vechile replici din revizia 3 si da revert back la revision 2, care ruleaza nginx:1.18
    -rulam si comanda describe sa fim siguri ca comanda rollout undo a dat switch la imaginea corecta, nginx:1.18 si asa este 
    -sa ne uitam acuma si la rollout history. Vedem ca inca avem 3 revizii in total. Este o revizie noua creeata acum, nr 4, dar revizia nr 2 nu mai exista 
    -s-a intamplat asta deoarece a 4 a revizie, care e revizia noua, este in mare parte la fel ca a doua revizie 
    -deci, efectiv, revizia 2 a devenit cea mai recenta revizie(ultima), dar si a schimbar nr reviziei din 2 in 4
    -si comanda de aici din CHANGE-CAUSE din history de la revizia 4 este aceeasi ca la revizia 2 

    -incercam alt scenariu, folosim comanda kubectl edit din nou si facem alte schimbari la deploymentu nostru

    $ kubectl edit deployment myapp-deployment --record

    -acuma stim ca imaginea ar trebui sa fie nginx:1.18 pt ca am dat revert la ultimu pas 
    -acuma facem o schimbare la containers dam comanda edit si la - imange: sa folosim o imagine care nu exista. Deci dupa - image: scriem: nginx:1.18-does-not-exist si dam save ctrl c si scriem :wq! 
!!!!-efectiv i am dat :w si dupa :qa si am lasat - din fata lu image si l am aliniat cu imagepullpolicy 
    -acuma dam comanda rollout status, si putem vedea ca statusu de rollout e blocat cu mesajul care zice: Waiting for deployment "myapp-deployment" rollout to finish si ca 3 din 6 replici au fost updated
    -o sa ramana blocat acolo deoarece incearca sa dea update la o imagine care nu exista
    -si nu o sa poata sa dea pull la acea imagine de pe docker hub, deci, containerele vor da fail 
    -deci, ca rezultat, nu o sa duca la bun sfarsit acest deployment 
    -daca dam comanda: vom vedea ca state ul deploymentului este de 5/6 replici 

    $ kubectl get deployment myapp-deployment 

    -rulam si comanda get pods, si vedem ca nu pods urilor care ruleaza este defapt 5, deci aici deploymentu a terminat 1 din pods urile existente(cu nginx:1.18) si a incercat sa faca 3 pods noi cu noua versiune a imaginii, cea care nu exista 
    -si de aceea sunt toate cele 3 intr un error state, ca numele imaginii pe care incearca sa o donwloadeze nu exista pe docker hubs
    -si ca rezultat, va tine deploymentu in stateu curent pana va putea sa dea pull la aceasta noua imagine dupa docker hubs 
    -deci, chiar daca am specificat imaginea gresita si avem 3 pods uri noi pe care incearca sa le dea load la imaginea noua, aplicatia nu e impactata si end userii vor putea totusi sa acceseze aplicatia pe cele 5 pods uri vechi 
    -asta se intampla datorita faptului ca am folosit strategia rolloing upgrade si kubernetes va downgrada sau distruge vechile pods doar cand avem suficiente pods uri noi disponibile 
    -daca ne uitam acuma la history, vom vedea o noua revizie, cea cu nr 5, este inregistrata

    -acuma ca un pas final sa facem o operatie de undo la un deployment ca sa treaca de la revizia cu nr 5 inapoi la revizia cu nr 4 si sa reparam numele imaginii inapoi la 1.18 
    -pt asta, folosim comanda rollout undo, si rulam comanda rollout status rpd si vedem ca e succesfull si dam get pods si vedem ca acela care trebuia recreat(ca erau 5 din 6) a fost creeat acu 5 secunde 

    $ kubectl rollout undo deployment myapp-deployment

    -si cele 5 pods uri care erau ramase aveau deja imaginea corecta 1.18 
    -si a mai dat destroy si la cele noi 3 pods, care incercau sa foloseasca iamginea gresita 
    -dam run la cmd describe si vedem ca aplicatia are acuma versiunea corecta nginx:1.18 si e totu ok 

Lab: Test Rolling Updates and Rollbacks 

    Task: We have deployed a simple web application. Inspect the PODs and the Services Wait for the application to fully deploy and view the application using the link called Webapp Portal above your terminal.

    $ kubectl get all 

    Task: What is the current color of the web application? Access the Webapp Portal.

    -blue 

    Task: Run the script named curl-test.sh to send multiple requests to test the web application. Take a note of the output. Execute the script at /root/curl-test.sh.

    $ curl-test.sh    //dam asta in terminal dupa ce vedem ca apre curl test in ls nu merge teapa 

    Task: Inspect the deployment and identify the number of PODs deployed by it

    $ kubectl get deployments 

    $ kubectl describe deployment frontend    //aici vedem la replicas ca s 4 

    Task: What container image is used to deploy the applications?

    - ne uitam sub pod template: sub containers: sub simple webapp: si vedem ca avem Image: kodekloud/webapp-color:v1 

    Task: What container image is used to deploy the applications?

    - ne uitam sus la strategytype si vedem ca ie rollingupdate 

    Task: If you were to upgrade the application now what would happen?

    - PODs are upgraded few at a time 

    Task: Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2 Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

    $ kubectl edit deployment frontend --record     // si schimbam la image din v1 in v2 

    Task: Run the script curl-test.sh again. Notice the requests now hit both the old and newer versions. However none of them fail. Execute the script at /root/curl-test.sh.

    $ ./curl-test.sh 

    Task: Up to how many PODs can be down for upgrade at a time? Consider the current strategy settings and number of PODs - 4

    - raspuns: 1

    Task: Change the deployment strategy to Recreate. Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.

    $ kubectl edit deployment frontend --record     // mergem la spec: la strategy: la type: scriem Recreate si stergem rollingUpdate: ramane doar asta:  dam :w si :qa 

    strategy:
      type: Recreate

    Task: Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v3 Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

    $ kubectl edit deployment frontend --record     // schimbam din v2 in v3 si facem ca mai sus :w si :qa 

    Task: Run the script curl-test.sh again. Notice the failures. Wait for the new application to be ready. Notice that the requests now do not hit both the versions Execute the script at /root/curl-test.sh.

    $ ./curl-test.sh 


Networking in Kubernetes 

    -incepem cu un cluster kubernetes care are singur NODE
    -nodeu are un ip adress(sa zicem 192.168.1.2) 
    -asta e adresa ip pe care o folosim sa accesam nodeu kubernetes, sa-i facem SSH, etc..
      -ca o paranteza, daca folosim un setup minikube, atunci vorbim despre adresa ip a masinii virutale minikube dinauntrul hypervisorului nostru 
      -laptopul nostru poate avea un ip diferit(192.168.1.10), deci e important sa intelegem cum sunt vm-urile noastre setate 
    -deci pe clusteru nostru kubernetes cu un singur node am creat un singur pod 
    -dupa cum stim, un POD hostuieste un container, spre deosebire de lumea Docker, unde o adresa ip este mereu asignata unui container docker, in lumea kubernetes, adresa ip este asignata unui POD 
    -fiecare POD in kubernetes primeste propria lui adresa ip interna 
    -in acest caz, este in rangeul seriei 10.244 si ipu asignat podului este 10.244..0.2  
    -deci cum primeste aceasta adresa ip?
    -cand kubernetes este configurat initial, creeam un internal private network cu adresa 10.244.0.0 si toate PODs sunt atasate la ea 
    -cand dam deploy la mai multe PODs, toate primesc un IP separat asignat din acest network(10.2440.3, 10.244.0.4)
    -PODs urile pot comunica intre ele prin acest IP, dar accesarea altor PODs folosind acest ip intern nu e o ideea buna deoarece se poate schimba cand PODs-urile sunt recreate 
    -vom vedea modalitati mai bune sa stabilim comunicare intre PODs imd 
    -acuma trei sa intelegem cum functioneaza internal networking in kubernetes 
    
    -e simplu de inteles cum merge networkingu pe un singur NODE, dar cum functioneaza cand avem mai multe NODEs in clusteru nostru?
    -in cazu acesta, avem doua nodes care ruleaza in kubernetes si au asignate ip urile: 192.168.1.2 si 192.168.1.3
    -notam ca nu sunt parte din cluster inca, fiecare din ele are un singur POD deployed, si cum am zis inainte, aceste PODs sunt atasate la un network intern si ele au propriile adrese IP asignate 
    -oricum, daca ne uitam la adresele network interne, vedem ca sunt aceleasi 
    -cele doua network uri au adresa 10.244.0.0 si cele doua PODs deployed au aceeasi adresa ip si ele: 10.244.0.2 
    -asta nu va fi ok cand NODEs sunt parte din acelasi cluster, PODs au acelasi ip asignat ceea ce va duce la conficte ip in network 
    -asta e o problema. Cand un cluster kubernetes este set up, kubernetes nu face set up automat la niciun fel de networking sa handleuiasca aceste probleme 
    -de fapt, kubernetes se asteapta de la noi sa facem set up la networking ca sa indeplinim anumite cerinte de baza 
    -unele dintre acestea sunt: -toate containerele sau PODs urile dintr-un cluster kubernetes trei sa comunice una cu alta fara sa trebuiasca sa aiba configurat NAT 
                                -toate NODEs-urile trebuie sa poata sa comunice cu containerele si toate containerele trebuie sa poata sa comunice cu NODEs-urile din cluster 
    -kubernetes se asteapta de la noi sa facem set up la o solutie de networking care indeplineste aceste criterii 
    -din fericire, nu trebuie sa-i facem set up-ul complet pe cont propriu, deoarece sunt multe solutii pre-built disponibile 
    -acestea pot fi: cisco ACI networks, cilium, big cloud fabric, flannel, vm ware nsx-t si calico 
    -in fucntie de platforma pe care dam deploy la clusteru kubernetes, putem folosi una din solutiile astea 
    -de ex, daca ar fi sa dam set up la clusteru kubernetes de la 0 pe propriile mele sisteme, as putea folosi oricare din solutiile: calico, flannel, etc..
    -daca am fi sa dam deploy pe un environment vm ware, NSX-T ar fi o solutie ok 
    -daca ne uitam la Play-with-k8s labs, ei folosesc wif net ca si solutia lor de netowrking 

    -ne intoarcem la clusteru nostru, cu custom networking cu calico sau flannel set up, acuma da manage la network-uri si ip-uri in my notes si asigneaza o adresa network diferita pt fiecare network din NODE 
    -asta creeaza un network virtual al tuturor PODS-urilor si NODEs-urilor, unde au toate asignate o adresa IP unica: nodurile au 10.244.0.0 si 10.244.1.0 si pods urile lor au: 10.244.0.2 si 10.244.1.2 
    -si folosind tehnici simple de routing, networkingul din cluster permite comunicarea dintre diferitele pods-uri sau nodes-uri ca sa indeplineasca astfel cerintele de networking ale lu kubernetes
    -astfel, toate PODs urile pot comunica acuma unul cu celalalt folosind adresa ip asignata

Services - NodePort

    -kubernetes services permit comunicarea intre mai multe componente in aplicatie si in afara acesteia 
    -kubernetes services ne ajuta sa conectam aplicatii impreuna cu alte aplicatii sau useri 
    -de ex, aplicatia noastra are grupuri de PODs-uri care ruleaza diferite sectiuni, de ex un grup pt deservirea load-ului de frontend userilor si alt grup pt rularea de procese backend si un al treilea grup care conecteaza la u sursa externa de date 
    -serviciile permit de fapt conectivitatea dintre aceste grupuri de PODs 
    -serviciile permit aplicatiei frontend sa fie disponibila userilor finali 
    -ele ajuta la comunicarea dintre podurile backend si cele frontend si ajuta in stabilirea conectivitatii la un data source extern 
    -astfel, serviciile permit cuplaju liber intre microserviciile din aplicatia noastra(adica comunicarea dintre front, back si data base)
    
    -sa vedem un use case al serviciilor
    -pana acu am vazut cum comunica PODs unul cu altul, prin internal networking, sa vedem si alte aspecte ale networkingului:
    -sa incepem cu external communication: deci am dat deploy la PODu nostru care are un webapp care ruleaza pe el. Cum noi ca un user extern accesam webpageu?
    -in primu rand sa vedem setupul existent: 
    -node-ul kubernetes are un ip care e: 192.168.1.2 
    -laptopul meu este pe acelasi network, de asemenea, deci are o adresa ip: 192.168.1.10 
    -networku intern al pod ului este in rangeu 10.244.0.0 si PODu are ipu: 10.244.0.2
    -este clar ca nu pot da ping sau sau sa accesez PODu cu adresa ip 10.244.0.2 pt ca este intr un network separat. deci care ie optiunile sa vedem webpageu? 
    -in primu rand, daca era sa facem SSH in NODEul kubernetes la 192.168.1.2 din NODE, am fi putut accesa pagina web a POD ului facut un curl http://10.244.0.2 , sau daca nodu are un DUI am fi pornit un browser si am fi vazut web pageu intr un browser la linkul htpp://10.244.0.2 
    -dar asta ar fi fost posibil dinauntrul NODEului kubernetes si nu vrem asta 
    -vrem sa putem accesa serveru web de pe laptopu nostru personal fara sa trebuiasca sa facem SSH in NODE ci doar simplu accesand IPu nodului Kubernetes(curl http://10.244.0.2)
    -deci avem nevoie de ceva in mijloc sa ne ajute sa mapam requesturi la NODE de pe laptopul nostru prin NODE la PODu care ruleaza containerul web 
    -aici intervine in schiema serviciul kubernetes 
    -serviciul kubernetes este un obiect, ca si PODs, ReplicaSets sau Deploymenturile cu care am lucrat inainte 
    -unul din usecase urile serviceului este sa dea listen pe un PORT de pe NODE si sa dea forward la requesturile de pe acel PORT pe un PORT al POD-ului pe care ruleaza aplicatia web 
    -deci, noi dam comanda de requeset(curl http://192.168.1.2:30008) de pe laptopu nostru(ca si useri) si aceasta ajunge la service-ul din interiorul NODE ului(care are acel ip) si la care serviceul asculta pe PORTul 30008(tot al NODEului) si serviceu va da forward la requesturile de pe portu 30008 la un PORT al POD ului care contine containerul docker pe care ruleaza webappu meu 
    -acest type de service este stiu ca un NODE PORT SERVICE deoarece serviciul da listen la un PORT de pe NODE si da forward la requesturi la PODs  
    -mai sunt si alte feluri de serivicii pe care le vom acoperi acum 

    -primul este cel pe care tocmai l am discutat (NodePort) in care serviciul face un port internal(care este al POD-ului) accesibil pe un port de pe NODE

    -al doilea este ClusterIP si in acest caz, serviciul creeaza un IP virtual inauntrul clusterului pt a permite comunicarea intre diferite serivcii, ca si un set de servere frontend care comunica cu un set de servere backend 
    
    -al treilea tip este un LoadBalancer, care ofera un Load Balancer pt aplicatia noastra in cloud providerii suportati. Un ex ar fi sa sa distribuim loadu pe diferite servere web in fontendu nostru

    -acum discutam de NodePort, despre care ziceam inainte de accessul extern la aplicatia noastra. Am zis ca un serviciu ne poate ajuta sa mapam un PORT de pe NODE la un PORT de pe POD
    -sa ne uitam mai indeaproape la Service. Daca ne uitam mai atent, vedem ca sunt 3 porturi implicate: PORT-ul de pe POD, unde ruleaza de fapt serviciul 80(si ipu era 10.244.0.2) si ne referim la acest PORT ca si TargetPort(1) deoarece acesta este portul unde serviciul da forward la request 
    -al doilea PORT este portul de pe serviciu si ne referim la acesta doar ca si Port(2), si este 80. Acesti termeni(TargetPort si Port) sunt din punctul de vedere al serviciului 
    -serviciul este de fapt ca un server virtual in NODE, in cluster, are propriul lui ip. Acest ip este numit Ip-ul cluster al service-ului.(10.106.1.12)
    -si in cele din urma, avem portul de pe NODE(portu nodeului propriuzis), pe care il folosim sa accesam serverul web din extern(ca un user) si acesta este stiut ca si NodePort - 30008 
    -dupa cum vedem, acest NodePort este 30008, deoarece NodePorts-urile pot fi doar intr-un valid range, care este by default de la 30000 la 32767 
    -Deci, avem nodeu care are ipu si portu 192.168.1.2:30008 si care ne duce la Serviceu NodePort, care are ipu 10.106.1.12 cu portu(Port) 80 si care acesta da forward la request pe POD care are portu cu ipu 10.244.0.2:80 (TargetPort - 80) si pe acest POD ruleaza containeru docker cu frontu web ului 
    
    -un Service il creeam cum am creeat si deployment-urile, replica set-urile sau pods-urile, vom folosi un definition file:
    -cele 4 proprietati high level raman la fel ca inainte, doar ca versiunea va fi v1, kind va fi Service, metadata va avea numele serviciului, poate avea si lable uri da nu ne trb acuma 
    -mai avem si spec: cea mai importanta parte a file ului pt ca asta este partea unde definim defapt serviciile si asta este partea definition file-ului care difera intre diferite obiecte
    -in seciunea spec: avem type si ports. type: se refera la tipul serviciului pe care il creeam, poate fi ClusterIP, NodePort sau LoadBalancer 
    -in cazu de fata il punem NodePort pt ca asta creeam 
    -urm parte din spec: este ports: aici punem informatiile pe care le am discutat inainte:
      -primul tip de port este TargetPort, pe care il setam la 80 (Portu Pod-ului unde serviceu forwardeaza requestu)
      -urmatorul se numeste simplu Port, pe care il setam la 80 si care este portul de pe Service(prin care serviceul forwardeaza requestu spre pod)
      -urmatorul este NodePort(protu nodului pe care il acceseaza useru), pe care il setam sa fie 30008, sau orice alt nr in rangeu respectiv 
      -de retinut este ca dintre toate aceste porturi, singurul mandatory este Port(cel al serviceului)
      -daca nu definim un TargetPort, acesta va fi la fel ca si Port(80)
      -si daca nu definim nici NodePort, un port liber din rangeu respectiv(30000 - 32767) este alocat automat 
      -notam si faptu ca ports: este un array, observam dash-ul (- targetPort) care indica primul element din array
      -putem avea mai multe astfel de port mappings intr-un singur service 
    
    -pana acum, nu e nimic in defintion file care sa conecteze serviceu cu POD-u 
    -am specificat doar TargetPort, dar nu am mentionat pe ce POD se afla acest TargetPort
    -pot fi sute de alte PODs cu web services care merg pe portu 80 
    -asa ca vom proceda cum am procedat si mai inainte cu ReplicaSets, e o tehnica pe care o vom vedea foarte des in kubernetes: vom folosi label-uri si selectoare sa link-uim serviceu de pod 
    -stim ca PODu a fost creeat cu un label. Trebuie sa aducem acel label in definition fileu serviceului 
    -deci, vom avea o noua proprietate in sectiunea spec: care se numeste selector: , ca si in replicaset si deployment 
    -sub selector: , dam o lista de label-uri pt a identifica POD-ul , pt asta ne uitam in def fileu folosit pt creearea POD-ului (de ex, app: myapp si/sau type: front-end)
    -luam acest(e) label(uri) din def fileu POD-ului si le punem aici, sub selector:
    -asta link-uieste serviceul cu POD-ul 
    -dupa ce e gata, creeam serviciul, folosind comanda kube control create si dam ca input cu optiunea -f service-definition.yml: 

    $ kubetl create -f service-definition.yml

    -sa vedem serviciul creeat, rulam comanda get services, care listeaza service-ul, cluster IPu, typeul, mapped ports:

    $ kubectl get services

    -type-ul este NodePort si portu de pe NODE e setat la 30008, deoarece acela este portul specificat in def file 
    -putem acuma sa folosim acest port sa accesam web serviceu folosind $ curl http://192.168.1.2:30008 sau direct pe browser cu linku 
    -ipu 192.168.1.2 este al NODE-ului si apoi folosim PORT-ul 30008, pt a accesa web serveru 

    service-definition.yml

    apiVersion: v1
    kind: Service 
    metadata: 
      name: myapp-service 

    spec: 
      type: NodePort
      ports:
      - targetPort: 80
        port: 80 
        nodePort: 30008 
      selector:
        app: myapp 

    -pana acuma am vb de un Service mapat la un singur POD. Ce facem cand avem mai multe PODs?
    -intr-un environment de productie, avem mai multe instante ale web app-ului nostru care ruleaza pt high availability si load balancing purposes 
    -in acest caz, avem mai multe PODs similare care ruleaza web appu nostru 
    -ele toate au aceleasi label-uri cu key-ul app si value-ul myapp (app: myapp) 
    -acelasi label este folosit ca un selector, in timpul creeari service-ului 
    -deci cand serviceu e creeat, cauta un matching pod cu label-ul si gaseste 3 astfel de pods uri(cele mentionate anterior 10.244.0.3 .2 .4)
    -atunci, Serviceu selecteaza automat toate cele 3 PODs ca Endpoint-uri pt a forwarda requesturile externe care vin de la useri(prin serviceu NodePort, cum am aratat mai devreme)
    -nu trebuie sa facem nicio configuratie aditionala sa se intample asta 
    -daca ne intrebam ce algoritm foloseste sa balanseze loadu pe cele 3 PODs-uri, foloseste un algoritm random 
    -astfel, Serviceu va actiona ca un load balancer built-in sa distribuie loadu pe pods urile noastre 
    
    -si in cele din urma, sa ne uitam ce se intampla cand PODs urile sunt distribuite pe mai multe NODEs 
    -in cazu asta, avem web appu pe PODs-uri pe NODEs-uri separate in cluster 
    -cand creeam un service fara ca noi sa trebuiasca sa facem configurari aditionale, kubernetes creeaza automat un Service care se intinde pe toate NODEs-urile din cluster si mapeaza TargetPortu sa fie acelasi NodePort pe toate NODEs-urile din cluster 
    -astfel, putem accesa aplicatia folosind oricare IP al oricarui NODE din cluster(192.168.1.2 /.3/.4) si folosind acelasi port number(portu NODE-ului), care in acest caz este 30008:

    $ curl http://192.168.1.2:30008
    $ curl http://192.168.1.3:30008
    $ curl http://192.168.1.4:30008

    -dupa cum putem vedea, folosind IPu oricaruia dintre NODEs-urile astea si incercand sa dam curl pe acelasi port(30008) 
    -si acelasi PORT(30008) este dsiponibil pe toate NODEs-urile care fac parte din cluster 

    -ca sa sumarizam, in orice caz, fie ca e un singur POD pe un singur NODE, mai multe PODS pe un singur NODE sau mai multe PODS pe mai multe NODES, serviceu e creeat exact la fel fara ca noi sa avem de facut pasi aditionali la creearea service-ului 
    -cand PODS-uri sunt sterse sau adaugate, serviciul este updatat automat, facandu-l foarte flexibil si adaptiv 
    -odata creeat, nu mai trebuie sa facem, de obicei, nicio schimbare de configurare aditionala

DEMO - NodePort 

    -in ultimu demo, am creeat un deployment care a creeat pods uri, sa-i verificam statusu:

    $ kubectl get deployment

    -deci avem deploymentu numit myapp-deployment care are 6 pods uri care merg in clusteru kubernetes 
    -deci auma avem o aplicatie care e creata sa ruleze pe acest cluster 
    -dar ca end useru sa poate sa o acceseze pe browseru lui, trebuie sa creeam un serviciu 
    -in editoru nostru, facem un nou folder numit service, si in acesta facem un nou file service-definition.yml. Structura asta de foldere e optionala, putem avea toate file urile in aceeasi locatie/fodler.
    -si ca si inainte, primul lucru care este in root element este apiVersion, care pt serviciu trebuie sa fie setat ca v1 (apiVersion: v1)
    -la kind: specificam Service 
    -la metadata: punem numele service-ului name: myapp-service 
    -sub asta adaugam sectiunea spec: si prima proprietate creeate va fi tipul serviciului, si anume: type: NodePort
    -scopul nostru este sa putem accesa aplicatia pe un port de pe NODE, care este nodeu minikube in cazu nostru 
    -si apoi vom adauga portul si portul default pe care asculta nginx, care e 80 
    -si ne vom adauga si targetPortu, care e tot 80 (asta este portu de pe Service)
    -si apoi adaugam un nodePort caruia ii dam valoarea 30004 (orice valoare in intervalu ala standard)
    -acest nodePort este portu de pe node, worker nodeu, care este nodeu minikube, pe care aplicatia va fi accesibila 
    -apoi ne facem un selector, care ne ajuta sa ne legam(binduim) serviceul la pod cu acelasi label 
    -verificam definition fileu pt deployment(deployment.yaml) si vom observa ca labelu pt pod este: app: myapp
    -deci vom adauga aceeasi label in fileu nostru, sub selector:  
    -dupa ce am facut si asta, definition fileu serviceului este gata si putem sa mergem sa creeam asta pe clusteru nostru 

    -ii dam save si ne intoarcem la terminal, in folderu service unde am creeat def fileu pt service 
    -creeam serviciul folosind comanda create cu optiunea -f si numele def fileului 

    $ kubectl create -f service-definition.yml    // rulam comanda si serviciul este creeat 

    -putem rula comanda get service si aici vedem ca nou serviciu este vizibil si tipul serviciului este NodePort, pt ca am vrut sa fie accesibil pe portu de pe nodu worker
    -si tot aici mai avem si ClusterIP(caracteristica a service-ului nostru, cu coloana ca si pt name, type, etc..) care este creeat pt serviciul nostru 
    -este o adresa ip creata pt serviceu din internal cluster network 
    -la PORT(S) avem portul de pe worker node pe care il putem folosi sa accesam aplicatia(30004 - este portu node-ului)
    -deci daca stim ipu nodolui woker, putem sa mergem efectiv pe un browser si sa scriem ipu node-ului worker, urmat de : si portu acestuia(:30004) si ar trebui sa putem sa accesam aceasta aplicatie 
    -acum, din moment ce rulam totu pe minikube, putem da comanda urm cu opt --url 

    $ minikube service myapp-service --url      // ne da ip si port diferit decat cele la care ne asteptam

    -mai putem sa dam si comanda urm, ca sa ne afiseze nodu si detaliile lui unde avem si internal ip 

    $ kubectl get nodes -o wide 

    -portul il stim deja si putem face asta dupa: http://<ipu>:<portu>
    -deci, vom avea asa: http://192.168.49.2:30004
    -cum suntem pe ubuntu, nu putem de pe un browser din windows sa accesam, dar putem cu comanda: 

    $ curl http://192.168.49.2:30004 si vedem ca ne afiseaza htmlu in care gasim welcome to nginx deci merge 

    ***-de aici inainte e mare workaroud care nu mere deci e optional, mai bn bag pe windows docker desktop cu minikube si intru direct din browser de pe windows pe web serveru respectiv

    -trb sa incerc si acasa pe windows toate astea si sa vad daca merge in browser 

    -ca sa ii dam expose sa putem accesa acest link dupa windows, dam comanda urm pt a avea ipu lui minikube: 

    $ minikube ip 

    -care ne va da 192.168.49.2 , portu il avem din comanda kubecetl get svc care ne da 80:30004 - 80 e portu serviceului si 30004 al node ului 

    -acuma trebuie sa dam expose la minikube vm pe windows 
    -de pe hostu windows putem sa facem set up la port forwarding sa forwardeze traficu de pe ipu windows la minikube nodePort
    -intram pe un powershell cu run as admin si dam comanda:

    netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=30004 connectaddress=192.168.49.2 connectport=30004

    -asta va da forward la traficu din windows la minikube 
    -ne asiguram ca firewallu de windows da voie la conexiuni incoming pe portu forwardat cu comanda:
    
    netsh advfirewall firewall add rule name="Minikube NodePort" dir=in action=allow protocol=TCP localport=30004

    -ne va da ok. dar tot nu mere. facem altfel:

    sudo apt install socat -y
    sudo socat TCP-LISTEN:30004,fork TCP:192.168.49.2:30004

    -daca dam comanda ip addr show eth0 , aflam ipu lu wls2:  172.17.34.243

    -dam comanda sudo socat TCP-LISTEN:30004,fork TCP:192.168.49.2:30004 si accesam http://172.17.34.243:30007

    -$ kubectl port-forward svc/nginx-svc 30004:80   sa abandoman socat ca tot nu mere si folosim asta 

    -tot nu merge, schimbam schimbarea: 
    $ kubectl patch svc myapp-service -p '{"spec":{"type":"LoadBalancer"}}' 
    ca sa facem load balancer din el 

    $ minikube tunnel 

    $ kubectl get svc myapp-service  gasim ipu extern si il bagam pe windows in browser 


Services - ClusterIP 

    -un app web fullstack are de obicei, diferite tipuri de PODs care hostuiesc parti diferite ale aplicatiei 
    -am putea avea un nr de PODs care ruleaza serveru de frontend, alt set de PODs care ruleaza serveru de backend, un set de PODs care ruleaza un key value store ca Redis si alt set de PODs care poate ruleaza o baza de date persistenta ca mySql 
    -serverele de frontend trei sa comunice cu serverele de backend si serverele de backend trei sa comunice cu baza de date si cu serviciile redis de asemenea 
    
    -deci care e metoda corecta sa stabilim conectivitatea dintre aceste servicii sau tire-uri ale app-ului nostru?
    -PODs urile au toate un IP asignat, dar aceste ip-uri nu sunt statice 
    -aceste POD-uri pot pica oricand si noi POD-uri sunt creeate mereu 
    -si deci, nu ne putem baza pe aceste adrese IP pt comunicare interna in cadrul aplicatiei 
    -sa zicem ca avem 3 pods-uri de frontend(ci ip uri 10.244.0.3/.2/.4) si 3 pods-uri de backend(cu ip-uri 10.244.0.5/.6/.7) 
    -daca primul pod frontend(cu.3 la finalu ip-ului) trebuie sa se conecteze la un pod de backend?
    -la care din cele 3 se conecteaza si cine face aceasta decizie?
    -un service kubernetes ne poate ajuta sa grupam pods-urile impreuna si sa ne ofere o singura interfata sa accesam PODs-urile dintr-un grup 
    -de ex, un service creeat pt pods-urile de backend va grrupa toate pods-urile de backend impreuna si sa ofere o singura interfata pt alte pods-uri sa acceseze acest service 
    -request-urile sunt forwardate catre unu din PODs-urile de dinauntrul serviciului random 
    -la fel, creeam un serviciu aditional pt Redis si permitem pods-urilor de backend sa acceseze sistemele Redis prin acest serviciu

    -asta ne permite sa dam deploy usor si sigur la o aplicatie bazata pe microservicii pe clusteru kubernetes
    -fiecare layer poate acuma sa dea scale sau move cum vrem, fara sa impacteze comunicarea dintre diferite servicii 
    -fiecare service primeste un IP si un name asignat acestuia inauntrul cluisterului si acela este numele care ar trebui sa fie folosit de alte PODs-rui sa acceseze service-ul 
    -acest tip de service e stiut ca si ClusterIP 
    
    -ca sa creeam una astfel de service ca de obicei, folosim un definition file in folderu service, in care folosim template-ul default care are cele 4 proprietati required: apiVersion, kind, metadata si spec, asa:

    service-definition.yml 

    apiVersion: v1 
    kind: Service 
    metadata:
      name: backend 
    spec: 
      type: ClusterIP
      ports:
      - targetPort: 80 
        port: 80
      selector: 
        app: myapp 


    -la spec: , pt type: valoarea default este ClusterIP, deci chiar daca nu specificam value-ul ClusterIP, type va fi automat ClusterIP 
    -sub ports: avem targetPort:(care e PORT-ul de pe POD) si port:(care e PORT-ul de pe Service)
    -targetPort: este portu pe care este expus backendu, 80 si port: este protu pe care este expus service-ul, care este 80 de asemenea 
    -ca sa link-uirm Serviceu la un set de Pods, folosim selector: 
    -ne vom referi la def fileu POD-ului si vom copia lable-urile de acolo si le dam paste sub selector: . Luam efectiv toate label-urile de sub metadata: labels: app: myapp si, daca exista, type: back-end sau tire: back-end...
    -acum putem creea Service-ul folosind kubectl si apoi sa i verificam statusu cu comanda kubectl get services 
    -serviciul poate fi accesat de alte PODs-uri folosind ClusterIP sau numele Service-ului 

    $ kubectl create -f service-definition.yml 

    $ kubectl get services 

Servives - LoadBalancer 

    -am vazut NodePort Service care ne ajuta sa facem o aplicatie accesibila extern pe un port pe worker nodes, printr-un port al serviceului din node, printr-un port de pe node(end userii sa se poata conecta la o aplicatie care ruleaza pe PODs urile noastre interne din cluster)
    -sa ne focusam pe frontend aplications, care sunt voting app si result app 
    -stim ca PODs-ruile de frontend(pt voting si result apps) sunt hostuite pe worker nodes din cluster 
    -sa zicem ca avem un cluster cu 4 NODEs 
    -si ca sa facem aplicatiile accesibile pt userii externi, creeam services de tipul NodePort
    -serviciile de tipul NodePort ajuta in primirea traficului pe porturile de pe NODEs si in rutarea traficului la respectivele PODs-uri 
    -dar ce URL le am da userilor end sa acceseze aplicatiile? 
    -am putea accesa oricare dintre aceste doua aplicatii folosind IP-ul Nodes-urilor si Port-ul de pe Nodes, care da forward la portul serviceului care da forward mai departe la portu POD-ului 
    -de ex: 192.168.56.70(71,72,73):30035 . deci, IP-urile Nodes-urilor sunt 192.168.56.70(71,72,73) si PORT-ul serviceului NodePort care forwardeaza traficu userilor pe Serviceu din Node si mai departe la PODu din Node pentru voting app 
    -de ex: 192.168.56.70(71,72,73):31061 pentru result app 
    -deci am avea in total 4 combinatii de IP-uri pentru voting app(ip urile de 2 randuri sus) si 4 combinatii de IP-uri pt restult app (un rand mai sus)
    -de notat este ca, chiar daca PODS-urile noastre sunt hostate doar pe 2 dintre NODES, ele vor fi totusi accesibile pe IP-urile tuturor Nodes-urilor din cluster 
    -sa zicem ca PODs-urile pt voting app sunt deployed doar pe NODES-urile cu IP-urile 70 si 71 
    -ele tot vor fi accesibile pe port-urile tuturor NODES-urilor de pe cluster(si 72 si 73) 

    -deci am putea share-uri aceste URL-uri userilor nostrii pt a accesa aplicatia, dar nu vor asta end userii
    -ei au nevoie de un singur URL, de ex: http://example-vote.com sau http://example-result.com pt a accesa aplicatia 
    -deci cum ajungem la asta?
    -un mod de a ajunge la asta este sa creeam un nou VM pt scopul de load balancer si sa instalam si configuram un load balancer potrivit pe VM, ca si ha proxy sau nginx 
    -apoi sa configuram load balancerul sa ruteze traficul la nodurile de sub el(cele 4 70,71...)
    -setarea acestui load balancing extern si apoi maintaining-ul si menegiuirea lui poate sa fie foarte complicat
    -oricum, daca am fi fost pe o platforma cloud suportata ca si Google Cloud sau Azure sau AWS am putea folosi load balanceru nativ de pe una dintre aceste platforme 
    -kubernetes are suport pt integrarea cu load balancerele native de pe anumite platforme de cloud providers si sa configureze asta pt noi 
    -deci, tot ce trebe sa facem este sa setam type-ul Serviceului pt serviciile frontend sa fie LoadBalancer in loc de NodePort 
    -tinem minte ca asta merge numai cu supported cloud platformes(Google Cloud sau Azure sau AWS)
    -deci, daca setam tipul serviceului sa fie LoadBalancer intr-un environment care nu e suportat ca si virtualBox sau orice alt environment,
    -ar avea acelasi efect ca si cum l-am fi setat pe NodePort, unde serviciile sunt exposed pe un Port high-end de pe Nodes
    -in cazu asta, nu va face niciun fel de external load balancer configuration 
    -mai incolo, cand vom face deploy la aplicatiile noastre pe cloud platforms, vom vedea asta in actiune 
    
    service-loadbalancer.yml 

    apiVersion: v1 
    kind: Service 
    metadata:
      name: myapp-service-loadBalancer 
    spec:
      type: LoadBalancer
      ports:
        targetPort: 80
        port: 80 
        nodePort: 30008
  

Coding Exercise - Services 

service-nodeport.yml 

apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: myapp
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30008
  selector:
    app: myapp

service-clusterip.yml 

apiVersion: v1
kind: Service
metadata:
  name: image-processing
  labels:
    app: myapp
spec:
  type: ClusterIP
  ports:
  - targetPort: 8080
    port: 80 
  selector:
    tier: backend     // in deployment-def.yml, la spec: template: metadata: labels: , avem tier: backend ca si la spec: selector: matchLabels:  tier: backend


Lab: Services 

    Task: How many Services exist on the system? In the current(default) namespace

    $ kubectl get services      // vedem ca exista doar 1 cu name kubernetes si type ClusterIP
                                // That is a default service created by Kubernetes at launch. Este un serviciu default cand se creeaza minikube, il avem si pe ubuntu nostru 

    Task: What is the type of the default kubernetes service?   
        - ClusterIP

    Task: What is the targetPort configured on the kubernetes service? 
    
    $ kubectl describe service kubernetes     // si vedem ca la TargetPort avem 6443/TCP 

    Task: How many labels are configured on the kubernetes service?

    -tot in comanda $ kubectl describe service kubernetes , ne uitam la Labels: si vedem ca avem 2 si anume component=apiserver si provider=kubernetes 

    Task: How many Endpoints are attached on the kubernetes service?

    -tot in comanda $ kubectl describe service kubernetes , ne uitam la Endpoints: si vedem ca avem unul singur: 192.168.129.239:6443 
    -acesta vine de la IP-ul POD-ului(poate fi vazut in describe pod) si PORT-ul POD-ului 6443(targetPort-ul care este portul POD-ului din def fileu serviciului, care poate fi fazut la describe service sau chair in def fileu serviceului respectiv)

    Task: How many Deployments exist on the system now? In the current(default) namespace

    $ kubectl get deployments     // vedem ca este 1 

    Task: What is the image used to create the pods in the deployment?

    $ kubectl get all     // si vedem ca avem 4 pods si dam describe la unu care vrem noi sa i vedem imaginea de baza a containerului din acesta 

    $ kubectl describe pod simple-webapp-deployment-646v7d6cd4-7n445    // vedem ca sub Containers: avem simple-webapp: si sub acesta avem Image: kodekloud/simple-webapp:red 

    Task: Are you able to accesss the Web App UI? Try to access the Web Application UI using the tab simple-webapp-ui above the terminal.

    -NO

    Task: Create a new service to access the web application using the service-definition-1.yaml file.

    Name: webapp-service
    Type: NodePort
    targetPort: 8080
    port: 8080
    nodePort: 30080
    selector:
      name: simple-webapp

    -nu putem accesa siteu web pt ca nu avem un serviciu sa putem accesa din exterior, gen end user, aplicatia noastra de pe pods urile interne 
    -asa ca facem un serviciu NodePort cu specificatiile de mai sus:

    $ sudo nano service-definition-1.yaml   // si scriem asa: si cam ctrl o si ctrl x dupa ce scriem 

    service-definition-1.yaml 

    apiVersion: v1
    kind: Service 
    metadata: 
      name: webapp-service
      namespace: default  
    spec: 
      type: NodePort
      selector:
        name: simple-webapp 
      ports:
      - nodePort: 30080
        port: 8080
        targetPort: 8080

    $ kubectl create -f service-definition-1.yaml 

    Task: Access the web application using the tab simple-webapp-ui above the terminal window.

    -acuma merge sa accesam webappu, cu un endpoint de la describe service webapp-service 


SSH from Windows to Ubuntu 

    $ ssh -L [local_port]:[minikube_ip]:[nodeport] <user_ubuntu>@[ubuntu_ip]    //comanda pt a realiza port mappingu local la cel de pe remote ubuntu machine sa putem accesa nginxu de pe ubuntu pe browser in windows gen 

    -local_portu trebuie sa fie un port liber dupa windows. Incercam cu acelasi port ca si nodePort de pe minikube si anume 30004(pe care il gasim daca dam kubectl get services sau describe la serviceu de tip NodePort), daca nu, incercam cu 8080:
    -pt afla daca ie liber dam comanda pe windows cmd:

    $ netstat -ano | findstr 30004     // :8080  - trebe sa nu afiseze nimic ca sa fie portu liber 

    -minikube ipu il aflam dand comanda urm in ubuntu: 

    $ minikube ip 

    -nodePort il aflam dand kubectl get services si vedem ce port avem la PORTS(TCP) la serviciu de tip NodePort saau dam describe la numele serviciului respectiv 
    
    -useru ubuntu il vedem in linia de comanda din ubuntu david@ROL... care este david 

    -ubuntu_ip il aflam dand comanda urm si ne uitam la eth0 la inet si e in cazu meu 172.17.34.243 

    $ ip addr show 


    -buon, acuma mergem la ubuntu si verificam daca serverul SSH este running cu comanda:

    $ sudo systemctl status ssh 

    -daca nu ruleaza il pornim:

    $ sudo systemctl start ssh 

    -sa i dam enable permanent dam:

    $ sudo systemctl enable ssh 

    -daca nu si nu, trei instalat:

    $ sudo apt update 

    $ sudo apt install opehssh-server 

    -dupa dam start si enable la server: 

    $ sudo systemctl start ssh 
    
    $ sudo systemctl enable ssh 

    -optional putem sa verificam daca permitem traficu prin firewall:

    $ sudo ufw allow ssh 

    $ sudo ufw reload 

    -acuma putem incerca sa facem SSH Tunnel din windows cmd:

    $ ssh -L [local_port]:[minikube_ip]:[nodeport] <user_ubuntu>@[ubuntu_ip] 

    -local port il avem, am vazut ca 8080 e liber cu comanda: $ netstat -ano | findstr: 8080
    -ipu minikube il aflam din linux cu comanda: $ minikube ip 
    -nodePort tot din linux dand describe la serviceu NodePort sau afisandu-l cu get 
    -useru din ubuntu e primu cuv din linia de comanda la mine e david 
    -ip u din ubuntu il aflam cu comanda: $ ip addr show si ne uitam la eth0 
    -astfel avem comanda urm pe care o executam in windows cmd: 

    $ ssh -L 8080:192.168.49.2:30004 david@172.17.34.243

    -acu ne intreaba de ampreuna unica de ecnriptie pt ca e prima oara cand dam ssh la ipu ubuntu 172.17.34.243 si dam yes dupa care parola pt useru ubuntu care e david 

    -daca totu a mers ok, intram pe browser in windows la adresa: http://localhost:8080 si trei sa ne arata Welcome to nginx! 

    ip minikube : 192.168.49.2

    node port: 30004

    http://192.168.49.2:30004 

    local port: 8080

    ipu ubuntu: 172.17.34.243 


Microservices Application 

    -vom folosi o simpla aplicatie facuta de docker pt a demonstra feauturile disponibile cand rulam un application stack pe docker - voting app 
    -este o aplicatie sample de votare care ofera o interfata pt user sa voteze si inca o interfata pt a arata rezultatele 
    -aplicatia are mai multe componente, ca si voting app, care este o aplicatie web facuta in python pt a oferi userului o interfata sa aleaga o optiune din doua, un caine si o pisica 
    -cand facem o alegere, votul este stocat in redis, care deserveste in acest caz ca o baza de date in memory 
    -acest vot este apoi procesat de worker, care e un app scris in .net. Acest app ia noul vot si updateaza baza de date persistenta, care este PostgreSql 
    -postgreSql are pur si simplu un tabel cu un numar de voturi pt fiecare categorie: caini si pisici. In cazu asta incrementeaza nr de voturi pt pisici pt ca asa am votat noi 
    -in cele din urma, rezultatul votului este afisat intr-o interfata web, care este o alta aplicatie web scrisa in nodeJs 
    -aceasta aplicatie resulting citeste numarul de voturi din baza de date postreSql si il afiseaza userului. Deci asta e arhitectura si data flow-ul acestei aplicatii de voting sample 
    -dupa cum putem vedea, aceasta aplicatie sample este construita printr-o combinatie de mai multe servicii, mai multe development tools si multiple development platforms ca si python, nodeJs, .net,...
    -acest sample aplication va fi folosita sa aratam cat de usor e sa facem set up la un intreg application stack care consta in mai multe componente in docker 
    -sa vedem cum putem pune impreuna acest stack pe un singur docker engine, folosind comenzi docker run 
    -sa zicem ca toate imaginile aplicatiilor sunt gata built-uite si disponibile pe docker repository 
    -prima oara rulam comanda docker run pt a porni o instanta de redis:

    $ docker run -d --name=redis redis 

    -adaugam optiunea -d pt a rula containeru in backround si ii punem un nume containerului: redis 
    -sa punem nume containerelor e important 

    -next, dam deploy la baza de date postgreSql ruland comanda:

    $ docker run -d --name=db postgres:9.4 

    -si de data asta folosim optiunea -d pt a rula in backround si denumim containerul db pt baza de date 
    
    -next dam deploy la aplication services, dam deploy la un frontend pt voting app, ruland o instanta a imaginii voting app. Rolam comanda docker run si denumim instanta vote:

    $ docker run -d --name=vote -p 5000:80 voting-app

    -deoarece este un web server, are o instanta web UI care ruleaza pe portu 80(portu containerului)
    -vom publica portu 8- la portu 5000 pe host ca sa il putem accesa de pe browser(facem port mapping)
    
    -next, dam deploy la results web app care arata rezultatele userului 
    -pt asta, dam deploy la un container care foloseste imaginea result app si publicam portu 80 al containerului pe portul 5001 de pe host:

    $ docker run -d --name=result -p 5001:80 result-app 

    -astfel, putem accesa web UI-ul al resulting app pe un browser 
    
    -finally, dam deploy la worker ruland o instanta a imaginii worker:

    $ docker run -d --name=worker worker 

    -acuma e totu ok si putem vedea ca toate instantele ruleaza pe host, dar nu putem sa ne conectam de pe browser la instanta de voting app folosind ip-ul host-ului si portul de pe host 
    -problema e ca am rulat cu succes toate containerele, dar nu le am si link-uit intre ele. Adica, nu i am zis aplicatiei web voting app sa foloseasca aceasta instanta specifica de redis. Pot fi mai multe instante redis care ruleaza 
    -nu i am zis worker-ului si resulting app-ului sa foloeasca aceasta baza de date specifica postgreSql pe care am pornit-o
    -Deci, cum facem toate astea?
    -DEPRECATED: --link este o optiune care poate fi folosita sa conectam doua containere impreuna, mai nou folosim un network in care le punem pe ambele containere si vor fi automat conectate unul la celalalt 
    -de ex, web serviceu voting app este dependent de service ul redis. Cand porneste serveru web pt voting app, cauta o instanta redis care e ready
    -dar, containerul voting app nu poate sa dea resolve un host cu numele "redis" 
    -pt a face voting appu sa stie de serviciul redis, adaugam o optiune link(sau le bagam in acelasi network) cand rulam voting app containeru pt a-l conecta la containerul de redis: --link redis:redis 
    -deci avem: $ docker run -d --name=vote -p 5000:80 --link redis:redis voting-app
    -in optiunea --link redis:redis , primul redis este numele propriu-zis al containerului redis, urmat de : si de numele host-ului pe care voting app il cauta(stim asta din codu lui sursa), care e tot redis in cazu asta(host="redis")
    -de asta am pus nume containerului cand l am rulat prima oara, ca sa ii putem folosi numelen in timp ce creeam un link. Asta defapt creeaza o intrare in etcd host file de pe containeru voting app, adaugand o intrare cu numele host-ului "redis" cu ipu intern al containerului redis 
    -la fel, adaugam un link pt result app sa comunice cu dbu postgres adaugand optiunea in comanda de rulare: --link db:db , sa ne referim la numele containerului postgres, "db" si la numele host ului pe care il cauta result appu(postgres@db) 
    -finally, worker app are nev de acces si la redis si la dbu postgres, deci ii adaugam doua link-uri, unu pt redis si altu pt db: $ docker run -d --name=worker --link redis:redis --link db:db worker (connectToRedis("redis"); connectToDB("db"))
    -e depreciat link si o sa fie sters pt ca concepte mai noi si avansate in docker swarm si in networking ofera suport pt cai mai bune de a ajunge la ce am facut noi adineaori cu links 
    

Microservices Application on Kubernetes 

    -am vazut cum e treaba pe docker, acu sa vedem cum ii dam deploy la app cu kubernetes 
    -notam ce plan avem 
    -Goal: Sa dam deploy la aceste aplicatii conteinarizate pe un cluster kubernetes si dupa sa dam enable la conectivitate intre containere ca aplicatiile sa se poata accesa una pe alta si bazele de date si apoi sa premitem accesul extern pt aplicatiile care au nevoie de acest acces, care sunt: voting app si result app, ca userii sa le poata accesa pe browser  
    -cum facem asta? 
    -stim ca nu putem da deploy la containere direct pe kubernetes. Am vazut ca cel mai mic obiect pe care il putem creea pe kubernetes este un Pod 
    -deci trebuie mai intai sa dam deploy la aceste aplicatii ca un POD pe clusteru nostru kubernetes. Sau le putem da deploy ca si replica sets sau deployments cum am vazut in curs 
    -dar la inceput, sa ne fie mai usor, ramanem la pods si dupa vedem cum sa trecem usor la un deployment de ex 
    -odata ce PODs-urile sunt deployed, urm pas e sa permintem conectivitatea dintre servicii 
    -trebuie sa stim exact care sunt cerintele de conectivitate. Adica ce serviciu trebuie sa aiba acces la care serviciu 
    -stim ca redis este accesat de voting-app so worker-app. Voting-app salveaza votul in redis si worker app citeste votu din redis 
    -stim ca postgreSql e accesat de worker app ca sa fie updatat cu numarul total de voturi si e accesat si de result-app sa citeasca numarul total de vouturi care sa fie afisat in resulting app in web page in browser 
    -stim ca voting app este accesat de useri externi(votantii) ca si result-app, ca sa vada rezultatele 
    -deci majoritatea componentelor sunt accesate de alta componenta, mai putin worker-app 
    -vedem ca worker-app nu e accesat de nimeni, el doar acceseaza alte servicii, dar niciun serviciu nu il acceseaza pe el, nici userii externi nu il acceseaza 
    -worker-app doar citeste nr de voturi din redis si apoi updateaza nr total de voturi din postgres. Deci nicio componenta interna(serviciu al aplicatiei) sau userii externi nu acceseaza niciodata worker-app.
    -in timp ce voting app are un web server de python care da listen pe portu 80, result app are de asemenea un server nodeJs care da listen pe portu 80 
    -si baza de date redis are un serviciu care da listen pe portu 6379 si bd-u postgresql are un serviciu care da listen pe portu 5432, 
!!! -worker-app nu are niciun serviciu, pt ca e doar un worker si nu e accesat de niciun al serviciu sau useri externi, tinem minte asta 

    -cum facem un component accesibil de catre un alt component, sa zicem de ex, cum facem baza de date redis accesibila de catre voting app?
    -voting-app NU ar trebui sa foloseasca IP-ul Pod-ului redis, pt ca acela se poate schimba daca Pod-ul se restarteaza 
    -si am putea avea probleme cand incercam sa dam sclae la aplicatii in viitor 
    -calea corecta sa o facem este prin folosirea unui serviciu. Am invatat ca un service poate fi folosit sa dea expose la o aplicatie altor aplicatii(ClusterIP) sau useri pt acces extern(NodePort)
    -deci vom creea un service pt POD-ul redis ca sa poata fi accesat de voting app si de worker app. 
    -il vom denumi redis service si o sa poata fi accesat de oriunde din cluster prin numele service-ului redis 
    -numele service-ului e important 
    -source codul din voting-app si din worker-app sunt hardcodate sa pointeze catre o baza de date Redis(host="redis", OpenRedisConnection("redis")) care merge pe un host cu numele redis 
    -de asta e important sa denumim service-ul redis. Ca aplciatiile sa se conecteze la baza de date redis 
    -nu e un best practice sa hardcodam chestii asa in source codu aplicatiei, in loc, am putea folosi environment variables sau ceva, acuma e hardcodat doar ca sa fie mai usor de inteles 
    -acuma, serviciile nu pot fi accesate in afara clusterului, deci ele ar trebui sa fie de tipul ClusterIP 
    -vom urmari aceeasi abordare de a creea un service pt POD-ul PostgreSql, ca PostgreSql DB sa poata fi accesat de catre worker-app si result-app 
    -daca ne uitam la codu result-app-ului si al worker-app-ului, vedem ca ele cauta o baza de date la adresa "db". Deci, service-ul creat pt baza de date postgreSql ar trebui sa fie numit "db" 
    -in timp ce ne conectam la baza de date, worker si result apps paseaza un username si un password pt a se conecta la baza de date(ambele sunt postgres, user si passu) 
    -deci, cand dam deploy la POD-ul postgres db, trebuie sa ne asiguram ca setam aceste credentiale pt el ca si setul initial de credentiale cand creeam baza de date 
    -urmatorul pas este sa dam enable la acces extern. Pt asta am vazut ca am putea folosi un service cu typeu NodePort 
    -deci creeam service-uri pt voting app si pt result app de tipul NodePort(2 servicii)
    -ne-am putea decide pe ce port sa le facem disponibile care ar fi un high port cu un numar mai mare de 30.000, vom face asta cand creeam service-ul 
    
    -asta e tot, avem pasii mari pregatiti. Ca sa sumarizam, vom da deploy la 5 PODs in total si vom avea 4 servicii: 1 pt redis si 1 pt postgres, ambele servicii interne de tipul ClusterIP, si apoi avem servicii externe pt voting si result app 
    -oricum, nu avem niciun service pt worker POD, deoarece nu ruleaza niciun serviciu care trebuie accesat de alt app(din cluster) sau useri externi 
    -deci este doar un worker process care citeste dintr o baza de date si o updateaza pe alta, deci nu va avea nevoie de un service 
    
    -un service este required daca aplicatia are un fel de proces sau un serviciu de baze de date sau un web service care trebuie sa fie exposed, care trebuie sa fie accesat de alte aplciatii din cluster sau useri externi 
    -in cazul nostru, worker app nu trebuie sa fie accesat de nimeni 

    -inainte sa incepem cu deploymentu, vom folosi urm imagini docker pt aceste aplicatii. Aceste imagini sunt facute dintr-un fork ale originalelor facute la docker samples repository 
    -numele imaginilor sunt kodekloud/examplevotingapp_vote:v1 , kodekloud/examplevotingapp_result:v1 , kodekloud/examplevotingapp_worker:v1 
    -pt databases vom folosi imaginile oficiale redis si postgresql care sunt disponibile 

DEMO - Deploying Microservices Application on Kubernetes 

    -am facut un nou folder numit Voting-App si primul lucru pe care il vom face este sa creeam pod definition files pt fiecare componenta din aplicatie 
    -sa incepem cu voting app insasi: facem un file in folder numit voting-app-pod.yml si il facem de la 0 si dupa la fel cu result-app:

    voting-app.pod.yml 

    apiVersion: v1 
    kind: Pod
    metadata:
      name: voting-app-pod
      labels:
        name: voting-app-pod
        app: demo-voting-app
    spec:
      containers:
      - name: voting-app 
        image: kodekloud/examplevotingapp_vote:v1
        ports:
          - containerPort: 80


    result-app-pod.yml 

    apiVersion: v1 
    kind: Pod
    metadata:
      name: result-app-pod
      labels:
        name: result-app-pod
        app: demo-voting-app
    spec:
      containers:
      - name: result-app 
        image: kodekloud/examplevotingapp_result:v1
        ports:
        - containerPort: 80

    -acuma sa creeam pod definition file pt pod-ul redis:

    redis-pod.yml 

    apiVersion: v1 
    kind: Pod
    metadata:
      name: redis-pod
      labels:
        name: redis-pod
        app: demo-voting-app
    spec:
      containers: 
      - name: redis 
        image: redis
        ports:  
        - containerPort: 6379     // portu default pt redis image

    -acuma sa creeam pod-ul pt database: 

    postgres-pod.yml 

    apiVersion: v1 
    kind: Pod
    metadata:
      name: postgres-pod
      labels:
        name: postgres-pod
        app: demo-voting-app
    spec:
      containers: 
      - name: postgres:9.4
        image: postgres
        ports:
        - containerPort: 5432
        env: 
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_PASSWORD
          value: "postgres"

    -la final, trebe sa adaugam sectiunea env:(environment variables) pt useru si parola bazei de date postgres, pt ca worker pod si result pod folosesc aceste credentiale cand se conecteaza la baza de date 
    -daca nu le adaugam, worker app nu o sa se poata conecta la baza de date si ca rezultat, countu total de voturi nu se mai incrementeaza 
    -deci in caz ca avem probleme cu vote countu sa nu se incrementeze si nu putem vedea rezultatele, putem verifica si aici 
    -acuma sa creeam si worker pod:

    worker-app-pod.yml 

    apiVersion: v1 
    kind: Pod
    metadata:
      name: worker-app-pod
      labels:
        name: worker-app-pod
        app: demo-voting-app
    spec:
      containers:
      - name: worker-app
        image: kodekloud/examplevotingapp_worker:v1     //nu ii punem ports pt ca nu are niciun port listening pt ca nu se conecteaza nimeni la el 
    
    -acuma vom creea services sa dam expose la pods, mai putin worker pod 
    -incepem cu serviciul pt redis:

    redis-service.yml 

    apiVersion: v1
    kind: Service
    metadata:
      name: redis
      labels:
        name: redis-service 
        app: demo-voting-app
    spec:                       // type: ClusterIP este optional pt ca typeu default este ClusterIP
      ports:
      - targetPort: 6379
        port: 6379
      selector:
        app: demo-voting-app     
        name: redis-pod         // ca sa link-uim serviceu la podu redis trei sa ii dam la selector aceleasi label uri ca ale pod-ului 

    -acuma creeam service-ul de postgres 

    postgres-service.yml 

    apiVersion: v1
    kind: Service
    metadata:
      name: db                    // pt ca worker app se asteapta ca numele bazei de date postgres sa fie db 
      labels:
        name: postgres-service 
        app: demo-voting-app
    spec:
      ports:
      - targetPort: 5732
        port: 5432
      selector:
        app: demo-voting-app     
        name: postgres-pod


    -acuma am terminat de facut cele doua internal services, sa le facem pe cele external: voting si result 
    -incepem cu voting app serive:

    voting-app-service.yml

    apiVersion: v1
    kind: Service
    metadata:
      name: voting-service
      labels:
        name: voting-service 
        app: demo-voting-app
    spec:
      type: NodePort
      ports:
      - port: 80
        targetPort: 80
        nodePort: 30004
      selector:
        name: voting-app-pod
        app: demo-voting-app

    -acuma il facem in mod similar si pe cel pt result app, ca si aceasta sa poata fi accesata extern de useri 

    result-app-service.yml 

    apiVersion: v1
    kind: Service
    metadata:
      name: result-service
      labels:
        name: result-service 
        app: demo-voting-app
    spec:
      type: NodePort
      ports:
      - port: 80
        targetPort: 80
        nodePort: 30005
      selector:
        name: result-app-pod
        app: demo-voting-app

    -fac un folder pe vmu meu linux si pun toate pod urile si service urile astea 
    -le vom porni unul cate unul si le vom testa sa ne asiguram ca merg cum trb 

    -ca sa creeam un pod bagam comanda create si punem numele fileului voting app pt inceput si la fel creeam si un service cu numele fileului voting service:

    $ kubectl create -f voting-app-pod.yml 

    $ kubectl create -f voting-app-service.yml 

    -ca sa verificam ca merg dam comanda: $ kubectl get pods,svc / all 
    -inainte sa mergem mai departe, sa testam sa vedem daca merge partea asta, putem sa accesam voting app service, folosind un url cu ipu lu nodeu minikube si portu 30004 pe un browser 
    -daca nu suntem siguri de ipu minikube putem da comanda urm sa ne dea dorect ipu si portu la care putem accesa voting service:

    $ minikube service voting-app-service --url     // care e cam teaca 

    $ netstat -ano | findstr 30004 // verificam daca 30004 e liber(acelasi port ca si nodePortu de pe kubernetes verificam daca e liber pe windows sa ne fie mai usor dar nu e si verificam 8080 pt ceva motiv):

    $ netstat -ano | findstr 8080   // e liber deci ii dam bataie cu asta inainte 

    $ ssh -L 8080:192.168.49.2:30004 david@172.17.34.243    // de pe win sa facem ssh la minikube 

    -acuma ne am putea conecta pe browser cu URL-ul: http://localhost:8080 care pointeaza pe ubuntu la urlu: http://192.168.49.2:30004
    -si vedem ca ne merge aplicatia pe web ne apare pagine de voting 
    
    -ne apucam sa creeam restu de servicii, ulrmatorul pod pe care il creeam este redis pod si apoi serviceu redis:

    $ kubectl create -f redis-pod.yml 

    $ kubectl create -f redis-service.yml 

    $ kubectl get all     // vedem ca sunt creeate si podu si serviceu redis 

    -acuma sa creeam podu postgres si serviciu aferent:

    $ kubectl create -f postgres-pod.yml 

    $ kubectl create -f postgres-service.yml 

    -acuma creeam worker pod:

    $ kubectl create -f worker-app-pod.yml 

    -acuma sa creeam podu si serviceu pt result app:

    $ kubectl create -f result-app-pod.yml 

    $ kubectl create -f result-app-service.yml 

    -cu get all vedem ca toate pods sunt up and running 
    -acuma putem vedea ca toate dintre cele 5 pods sunt running si avem si ca avem 2 servicii node port(unu pt result si unu pt voting service). Celelalte doua servicii create sunt redis si database service care is internal 
    
    -doar ce am accesat aplicatia voting, sa generam urlu si pt resulting app 
    
    -acuma incepem workaroundurile sa facem ssh si la result app care e pe portu 30005 
    
    $ minikube service result-service --url 

    -daca dam comanda asta ne da: http://192.168.49.2:30005
    -deci tre sa facem ssh local iarasi cred la ipu si porturile astea 
    -acuma tre sa verificam daca portu 30005 e liber pt ca am verificat daca 30004 era liber si nu era si dupa am incercat cu 8080 care era liber dar acu e folosit pt sshu de adineaori 
    -deci dam comanda: 

    $ netstat -ano | findstr 30005

    -vedem ca nu afiseaza nimica deci e bines . Daca era si asta ocupat verificam cu 8081 care aparent e liber si el. Deci comanda va fi asea:

    $ ssh -L 30005:192.168.49.2:30005 david@172.17.34.243   
    
    -merge, deci urlu pt pagina web result app va fi: http://localhost:30005 
    -intram pe voting app si votam dogs de ex si vedem un checkmark semn ca votu nostru a fost recorded si salvat in redis data base 
    -daca mergem pe results page acuma vedem ca dogs au acuma 100$ din voturi, pt ca avem un singur vot si ala la dogs
    -putem schimba votu pe cats si vedem ca se schimba si result app web page!!! yay deci datele merg de la front end la back end pron redis bazadate prin worker pod prin postgres si tocmai la celalalt front end
    
DEMO - Deploying Microservices Application on Kubernetes with Deployments 

    -tocmai ce am dat deploy la o aplicatie prin pods si services, dar asta nu e cea mai buna varianta 
    -sa dam deploy la pods nu ne ajuta sa scalam aplicatia usor, daca vrem sa adaugam mai multe instante ale unui serivce 
    -daca am vrea sa updatam aplicatia, adica sa updatam o imagine care folosea aplicatia, atunci trei sa dam aplicatia jos cat timp este creeat noul pod cu noua imagine, deci o sa avem un downtime 
    -deci approach-ul cel mai bun este sa folosim deployment-uri sa dam deploy la o aplicatie 
    -acuma facem set-up ul de data trecuta cu deployments-uri 
    -alegem deployments peste replcia sets, pt ca deployment-urile creeaza replica sets automat daca trebuie si ne pot ajuta sa facem rolling updates sau rollbacks si sa avem un record al reviziilor si record la cauza schimbarii cum am vazut in demo-urile anterioare 
    -vom adauga mai multe pods daca e nevoie la aplicatiile de frontend(la voting app si result app) creeand un deployment si punand replicasets sa fie 3, vom incepe initial cu doar o replica pt fiecare dintre componente si mai tz vom vedea cat de usor e sa le scalam la 3 sau mai multe 
    -vom incapsula in deployment si bazele de date si worker appu 

    -creeam un nou file pt deployments. Incepem cu deploymentu pt voting app:
    -puntem metadata ca si la celelalte pods/services, la spec punem 1 replicaset, la selecctor la matchLabels punem lable-urile de la labels: ale voting app pod 
    -la secu de la templateu de pod punem exact ce regasim si la specu de la voting app pod 
    -cu alte cuvinte, sub template-u din deploy punem tot ce are pod def fileu de la metadata in jos inclusiv:

    voting-app-deploy.yml 

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: voting-app-deploy
      labels:
        name: voting-app-deploy
        app: demo-voting-app
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: voting-app-pod
          app: demo-voting-app   # punem labelu folosit si la podu voting-app-pod.yaml
      template:
        metadata: 
          name: voting-app-pod
          labels:
            name: voting-app-pod
            app: demo-voting-app   # trebuie sa fie la fel cu labelu din selector: cel de sus sus nu cont
        spec:
          containers:
          - name: voting-app 
            image: kodekloud/examplevotingapp_vote:v1
            ports:
            - containerPort: 80


    redis-deploy.yml 

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: redis-deploy
      labels:
        name: redis-deploy
        app: demo-voting-app
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: redis-pod
          app: demo-voting-app   # punem labelu folosit si la podu voting-app-pod.yaml
      template:
        metadata: 
          name: redis-pod
          labels:
            name: redis-pod
            app: demo-voting-app   # trebuie sa fie la fel cu labelu din selector: cel de sus sus nu cont
        spec:
          containers:
          - name: redis 
            image: redis
            ports:
            - containerPort: 6379
    

    postgres-deploy.yml

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: postgres-deploy
      labels:
        name: postgres-deploy
        app: demo-voting-app
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: postgres-pod
          app: demo-voting-app   # punem labelu folosit si la podu voting-app-pod.yaml
      template:
        metadata: 
          name: postgres-pod
          labels:
            name: postgres-pod
            app: demo-voting-app   # trebuie sa fie la fel cu labelu din selector: cel de sus sus nu cont
        spec:
          containers:
          - name: postgres 
            image: postgres:9.4 
            ports:
            - containerPort: 5432
            env: 
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              value: "postgres"


    worker-app-deploy.yml 

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: worker-app-deploy
      labels:
        name: worker-deploy
        app: demo-voting-app
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: worker-app-pod
          app: demo-voting-app   # punem labelu folosit si la podu voting-app-pod.yaml
      template:
        metadata:
          name: worker-app-pod
          labels:
            name: worker-app-pod
            app: demo-voting-app
        spec:
          containers:
          - name: worker-app
            image: kodekloud/examplevotingapp_worker:v1


    result-app-deploy.yml 

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: result-app-deploy
      labels:
        name: result-app-deploy
        app: demo-voting-app
    spec:
      replicas: 1
      selector:
        matchLabels:
          name: result-app-pod 
          app: demo-voting-app   # punem labelu folosit si la podu voting-app-pod.yaml
      template:
        metadata:
          name: result-app-pod
          labels:
            name: result-app-pod
            app: demo-voting-app
        spec:
          containers:
          - name: result-app 
            image: kodekloud/examplevotingapp_result:v1
            ports:
            - containerPort: 80

    -sa zicem ca nu mai avem nimic care ruleaza pe ubuntu si incepem sa dam deploy la tot:

    $ kubectl create -f voting-app-deploy.yml 
    $ kubectl create -f voting-app-service.yml

    $ kubectl create -f redis-deploy.yml 
    $ kubectl create -f redis-service.yml

    $ kubectl create -f postgres-deploy.yml 
    $ kubectl create -f postgres-service.yml

    $ kubectl create -f worker-app-deploy.yml

    $ kubectl create -f result-app-deploy.yml 
    $ kubectl create -f result-app-service.yml


    $ ssh -L 8080:192.168.49.2:30004 david@172.17.34.243      //facem ssh de pe win pe ubuntu de pe portu 8080 de pe win pe 30004 pe ubuntu cu ipu aferent ubuntu 

    $ ssh -L 30005:192.168.49.2:30005 david@172.17.34.243 

    -le accesam prima pe localhost:8080 si a doua pe localhost:30005

    -acuma dam scale up la deployment sa avem 3 replici:

    $ kubectl scale deployment voting-app-deploy --replicas=3 --record 

    -dam get all si vedem ca avem 3 pods pt voting app
    -daca intram pe pagina de voting si dam refresh, vedem ca pagina va fi practic hostuita de un pod diferit de fiecare data(unul dintre cele 3)
    -deci e foarte usor sa dam scale la aplicatii cu deployments 


Kubernetes on AWS 

    -o sa creeam un EKS Cluster, care e serviciul menegiuit de Amazon pt Kubernetes 
    -ne trb cont aws, avem 
    -trebe sa avem KubeCtl CLI instalat https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/
    -ne mai tre EKS Cluster Role, IAM Role pt Node Group, un VPC, un EC2 Key Pair sa dam ssh in worker nodes, si si AWS CLI si basics de AWS. merem aici https://docs.aws.amazon.com/eks/latest/userguide/install-awscli.html
    -mergem la IAM Dashboard si dam pe User si alegem Security credentials la access keys dam Create access key , dupa alegem Command Line Interface si Next ii dam Create access key si ii dam download. Tot aici vom avea si access key si secret access key 
    -dam comanda urm. intr un cmd dupa ce il instalam https://awscli.amazonaws.com/AWSCLIV2.msi si bagam access key id si secret access key id pe care le avem dupa ce donwloadam keya creeata intr un excel 

    $ aws configure

    -aws sts get-caller-identity sa vedem ca e ok dupa ce am dat configure 
    -instalam si eksctl.exe  https://eksctl.io/installation/

    https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html

    -urmam tutorialul pas cu pas si vedem ca o sa ne iasa insfarsit, trebuiesc creeate rolurile cu fisiere json de config si atasate permisiunile cu comenzi de la tastatura dar merita efortu boss 
    -facem un VPC cu template-ul din tutorial(https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-vpc-private-subnets.yaml)
    
    $ kubectl get nodes     /pt a vedea cele doua nodes de tipul ec2 creeate ca sunt running si ca suntem conectati din cmd la clusteru kubernetes  

    -mergem intr un cmd din locatia folder-ului voting-app si ne apucam sa creeam deployment-urile si service-urile cu comenzile aferente kubectl cum am facut si mai inainte:
    -inainte sa dam deploy la services le facem pe cele doua service-uri pt frontend adica cel pt Voting-app si cel pt Result-app LoadBalancer in loc de NodePort 
    -facem asta deoarece le dam deploy pe AWS si vor folosi sistemul de load balancing de la AWS si ca sa poata face asta, trebuie sa avem servicii de tipul LoadBalancer 

    $ kubectl create -f voting-app-deploy.yml 
    $ kubectl create -f voting-app-service.yml

    $ kubectl create -f redis-deploy.yml 
    $ kubectl create -f redis-service.yml

    $ kubectl create -f postgres-deploy.yml 
    $ kubectl create -f postgres-service.yml

    $ kubectl create -f worker-app-deploy.yml

    $ kubectl create -f result-app-deploy.yml 
    $ kubectl create -f result-app-service.yml

    -verificam totul cu comanda 

    $ kubectl get all 

    -si vedem ca la services avem cele doua servicii cu noile tipuri LoadBalancer modificate anterior 
    -la external ip avem afectiv link urile de la aws de la delpoyment-urile noastre care sunt accesibile end userilor 
    -daca le luam cu copy paste si le bagam in borwser vedem ca o sa mearga 
    -stergem tot sa nu ne haleasca awsu cum a facut o deja. incepem prin a sterge node groups si dupa stergem clusteru cu totu 

Setup Multi Node cluster using Kubeadm 

    -kube admin tool poate fi folosita sa dam bootstrap la un cluster kubernetes 
    -kube admin tool ne ajuta sa facem set up la un cluster multi-node, folosind kubernetes best practices 
    -clusteru kubernetes consta in mai multe componente: kube api server, etcd, controllere etc 
    -am vazut reqirement-urile pt security si certifications ca sa permitem comunicarea intre toate componentele  
    -instalarea tuturor componentelor in parte pe diferite nodes si sa modificam toate configuration file-urile necesare sa ne asiguram ca toate componentele pointeaza una la alta si sa facem set up la certificate sa facem sa mearga totu este un task migalos 

    -kube admin tool ne ajuta avand grija de toate aceste task-uri 
    -sa trecem prin pasii de a face set up la un cluster de kubernetes folosind kube admin tool la un nivel inalt 
    -in primu rand, trei sa avem multiple sisteme sau VM-uri provizionate, deci o sa avem nevoie de cateva nodes pt clusteru kubernetes care pot fi fizice sau vm-uri 
    -odata ce toate nodes-urile sunt porvizionate, o sa trebuiasca sa desemnam unul ca si master si restul ca si worker nodes 
    -urmatorul pas este sa instalam un container runtime pe host-uri si vom folosi in specific containerD si deci vom instala containerD pe toate nodes-urile 
    -apoi, va trebui sa instalam kube admin tool pe toate nodes-urile ca kube admin tool sa ne ajuta sa facem boostrap la solutia kubernetes prin instalarea a tuturor componentelor necesare pe node-urile corecte in ordinea corecta 
    -dupa aceea, va trebui sa initializam serverul master, deci in timpul acestui proces, toate componentele required sunt instalate si configurate pe master 
    -apoi, dupa ce am initializat serverul master, dar inainte ca node-urile worker sa dea join pe cluster, trebuie sa ne asiguram ca pre-cerintele(preqrequisites) de network sunt intalnite 
    -deci conectivitatea retelistica normala nu e indeajuns pt asta(normal network connectivity is not enough for this)
    -kubernetes are nevoie de o solutie speciala de networking intre nodul master si nodurile worker, numita POD Network 
    -si deci, odata ce POD Network este set up, avem tot ce ne trebe ca sa putem sa dam join la nodurile worker la nodul master
    -si odata ce au dat join la nodul master, putem sa mergem sa dam deploy la aplciatia noastra pe environmentul kubernetes 

DEMO - Setup Lab - VirtualBox 

    -acum vom vedea cum sa provizionam vm-uri pt clusteru nostru kubernetes care vor include un master si doua workere 
    -ne vom folosi de doua soft-uri, primul este VirtualBox - care este hypervisor-ul nostru 
    -si cel de al doilea este vagrat -care e ca un automation tool care face foarte usoara treaba sa dam spin up la mai multe vm-uri cu o configuratie specifica 
    -deci, ne vom folosi de Vagrant ca sa avem toti aceeasi configuratie pt VM-uri si ca sa putem porni acele VM-uri doar cu o singura comanda 
    -deci, prerequisites-urile sunt sa instalam VirtualBox si Vagrant 
    -mergem la oracle virtual box la downloads si selectam OS-ul nostru si urmam pasii 
    -apoi pt Vagrant, mergem la documentatia lor, unde au un walkthrough numit Install Vagrat
    -pt Vagrant avem un vagrant file care va contine toate configuratiile noastre pt VM-urile noastre. Acest file a fost deja scris pt noi pe care o putem gasi la repou cursului kodekloudhub/certified-kubernetes-administrator-course 
    -mergem si dam clone la repo ca sa avem access la vagrant file pe care o folosim sa dam spin up la vm uri 
    -ca sa clonam repou, megem pe el si acolo la dropdownu-ul Code copiem linku si rulam un git clone si dam paste la url si asta va clona repo-ul sau dam download la zip direct si aia e 
    https://github.com/kodekloudhub/certified-kubernetes-administrator-course

    -in kubernetes clusters avem virtual box si in acel folder avem vargat file. Dar tre sa instalam virtual box si vargant ca sa putem continua, dam si restart la pc sa si mearga vargant 

    -acuma ca le am instalat, mergem in folderu kubernetes si dupa in folderu kubeadm, unde avem Vargantfile-ul 
    -dam aici comanda: 

    $ vagrant status 

    -o sa ne zica ca cele 3 vm uri 2 pt workers si 1 pt master node nu sunt create 
    -asa ca le vom creea cu comanda urm, care va da provision la toate cele 3 vm-uri cu specificatiile exacte din vagrant file:

    $ vagrant up 

    -o sa dea pull la imaginea ubuntu/jammy64 si odata ce imaginea e pulled o sa faca toate cele 3 vm uri dar o sa dureze mult asa ca trei sa stam sa asteptam vedem ca se blocheaza unpic da putem deschide virtual box si intra dinrect in controlplane si cumva isi da drumu sa mearga mai departe 
    -acuma dupa ce avem vm urile facute putem da comanda $ vagrant status , si vom vedea ca toate cele 3 nodes sunt intr-un running state, ceea ce e ce voiam 
    -acum ca le avem running pe toate 3 nodes-urile, cum ne conectam la ele? 
    -dam comanda vagrant ssh urmata de numele node-ului la care vrem sa ne conectam, de ex controlplane 

    $ vagrant ssh controlplane 

    -asta ne va conecta automat la NODE-ul controlplane(node-ul master)
    -vedem dupa prompt-ul command line-ului ca suntem bagati in node(vagrant@kubemaster:~$ ) si daca dam un $ ls - la  , vedem ca suntem in controlplane 
    -ca sa dam Logout, ca sa iesim din Node pur si simplu dam logout si ne aduce inapoi la local machineu nostru: 

    $ logout 

    -si daca mai dam un vagrant status, si dam iar ssh dar de data asta la node01, si daca dam un $ uptime , vedem ca nu e pornit demult timp, si suntem conectati la node01 
    -ne dam logout si de aici si ne conectam si la node02 

    -deci, avem toate VM-urile pregatite si provisioned si suntem ok sa mergem mai departe. Deci in video-ul urmator o sa dam boostrap la clusteru kubernetes folosind kubeadm 


Demo - Provision cluster using Kubeadm 

    -ne am creat vm-urile car vor actui ca si nodes-urile noastre necesare pt a incepe sa ne creeam clusteru kubernetes 
    -avem 3 nodes, unul master si doua workers, putem deschide in visual studio 3 terminale in care sa ne conectam la fiecare din ele cu vagrant ssh 
    -mergem la documentatia kubernetes pt a instala kubeadm  https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
    -*optional* verificam ca VM urile au adrese MAC si product_uuid unice cu comenzile $ ip link si $ sudo cat /sys/class/dmi/id/product_uuid

    -deci va trebui sa instalam un container runtime pe toate NODEs-urile si mergem aici https://kubernetes.io/docs/setup/production-environment/container-runtimes/
    -vom folosi containerD , dar pana acolo va trebui sa facem niste configurari de networking
    -by default, kernelu de linux nu permite pachetelor IPv4 sa fie rutate intre interfete. Deci, ca sa permintem fowrading-ul de pachete IPv4 manual dam copy paste la comenzile urm in toate VM-urile: 

    # sysctl params required by setup, params persist across reboots
    $ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 EOF

    # Apply sysctl params without reboot
    $ sudo sysctl --system

    -mare atentie mai bine scriem manual fiecare comanda pe linie si dam enter:
    -deci incepem asa: $ cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf   // scrieam asta si dam enter 
    -o sa ne bage in cat editor si o sa apara doar > dupa care scriem asta: > net.ipv4.ip_forward = 1
    -acuma o sa apara iar > pe linie noua si scriem: > EOF ca sa ne scoata dinb editor 
    -dupa care dam comanda: $ sudo sysctl --system
    -practic rescriem un file de configurare kubernetes si docker cred dupa ii dam apply cumva fara sa dam reboot la vm 
    -ca sa verificam, putem da comanda: $ sysctl net.ipv4.ip_forward

    -dupa mergem mai jos la containerd si dam click pe linku getting started with containerd care ne duce pe un link de github de containerd 
    -aici mergem mai jos la Option 2 si alegem Ubuntu care ne va duce pe pagina oficiala de documentatie Docker pt a instala docker engine pe ubuntu 
    -si mergem mai jos la Install using the apt repository si vedem urm set de comenzi pe care le dam rand pe rand la fiecare vm(le dam copy dupa dam click pe terminal si dam click dreapta sa le dea paste altfel ii da paste aiurea):

    # Add Docker's official GPG key:
    sudo apt-get update
    sudo apt-get install ca-certificates curl       // primele doua comenzi dau set up la repository 
    sudo install -m 0755 -d /etc/apt/keyrings
    sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc    
    sudo chmod a+r /etc/apt/keyrings/docker.asc           // aceste comenzi adauga Docker official GPC keys 

    # Add the repository to Apt sources:
    echo \
      "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
      $(. /etc/os-release && echo "${UBUNTU_CODENAME:-$VERSION_CODENAME}") stable" | \
      sudo tee /etc/apt/sources.list.d/docker.list > /dev/null  
    sudo apt-get update                             // acuma dam set up la repository si cu ultima comanda dam un apt update 

    -ultimul pas este sa dam comanda in care instalam containerd, pe pagina docker apare comanda cu toate pachetele pt docker, dar noiv rem sa instalam doar containerD
    -deci comanda, in loc sa arate asa: sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin , va arata asa: 

    $ sudo apt install containerd.io

    -dupa ce s-a instalat si asta, putem da comanda pt a verifica instalarea lu containerD 

    $ systemctl status containerd   // unde vedem ca e actively running 

    -mai tre sa facem o chestie cu containerd, daca dam scroll up la pagina initiala de kubernetes cu Container Runetimes unde am rulat comenzile prerequisites si mergem la cgroup drivers 
    -pe machine-urile linux exista control groups care sunt folosite sa dea constrain la resurse care sunt alocate la diferite procese care sunt running pe machineu noastru 
    -si kubelet si container runtime au nevoie sa interfereze cu aceste control groups sa dea enforce la diferite resurse de management pt pods 
    -de ex, sa seteze request-urile si limitele pt CPU si Memory 
    -acestea sunt lucruri care trebuiesc comunicate cgroups-urilor si deci, kubelet si container runtime trebe cumva sa interactioneze si sa interface cu aceste cgroups 
    -si ca sa interactiuneze cu aceste container grouops, kubelet si container runetimes trebuie sa foloseasca un cgroup dirver 
    -si sunt doua drivere diferite: cgroupfs si systemd 
    -cgroupfs este de obicei default-ul si celalalt este systemd
    -depinde de mai multe lucruri pe care il folosim
    -dar regula este ca, daca folosim un systemd init system, trebuie sa folosim driveru systemd
    -deci, asta ar insemna ca ar trebui sa il schimbam, deoarece cgroupfs este cel default pt container runtime 
    -si pe langa asta, oricare driver e folosit de kubelet, acelasi trebuie folosit si de container runtime sau invers 
    -deci, daca container runtime foloseste driveru systemd, atunci kubelet trebuie sa foloseasca tot systemd 
    -nu putem avea unul care sa foloseasca cgroupfs si celalalt sa foloseasca systemd, trebuie sa foloseasca ambele acelasi driver 
    
    -le vom seta pe ambele sa fie systemd 
    -asa ca sa mergem sa il setam pt container runtime 
    -dar in primul rand sa verificam care este init systemul nostru 
    -e foarte simplu sa facem asta, trebuie sa listam procesele si sa l alegem pe cel cu id-ul 1 si acesta va fi init system-ul nostru:

    $ ps -p 1 

    -si vedem ca foloseste systemd si deci trebuie sa setam cgroup driver la systemd 
    -ca sa facem asta, mergem mai jos pe pagina la containerd si mai jos avem Configuring the systemd cgroup driver
    -aici ne zice ca trei sa mergem la /etc/containerd/config.toml  si tre sa configuram asta: 

    [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
        SystemdCgroup = true

    -luam asta cu copy paste dupa ce scoatem cele 3 ... cum apare pe net si dam comanda:

    $ sudo vi /etc/containerd/config.toml 

    -si asta e important!!! dam delete la toate configurarile default, altfel vom avea erori 
    -apoi dam paste in la configuratia noastra de mai sus: plugins...  si dam :w apoi dam :qa 
    -dupa ce dam save la file si verificam ca s a si salvat si dam restart la containerd cu comanda ca sa se aplice schimbarile:

    $ sudo systemctl restart containerd 

    -trei sa facem asta pt toate vm-urile!!!

    -dupa ce facem asta la toate vm-urile revenim pe pagina initiala https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
    -aici am terminat sectiunea Installing a container runtime
    -si mergem la urmatoarea in care vom instala: Installing kubeadm, kubelet and kubectl

    -kubeadm e tool-ul care e responsabil pt boostrap-ingul clusterului
    -kubelet e componenta care ruleaza pe toate clusterele noastre care va face lucruri ca pornirea PODS-urilor si a Containerelor 
    -kubectl este command line utility pe care o folosim sa vorbim cu clusteru 
    
    -mergem mai jos in pagina si la Debian-based instructions vedem ca sunt toti pasii pt a instala kubeadm 
    -doar tre sa dam copy paste la comenzile urm pe toate cele 3 nodes:

    $ sudo apt-get update
    $ sudo apt-get install -y apt-transport-https ca-certificates curl gpg

    $ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    $ echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

    $ sudo apt-get update
    $ sudo apt-get install -y kubelet kubeadm kubectl
    $ sudo apt-mark hold kubelet kubeadm kubectl

    $ sudo systemctl enable --now kubelet     // **optional

    -acuma sa incepem sa configuram clusteru nostru kubernetes . Dam in josu paginii la Whats next pe link si ne duce aici https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
    -mergem la sectiunea Initializing your control-plane node
    
    -primul pas cu comanda --control-plane-endpoint este doar daca o sa rulam cu mai multe node-uri Master, dar noi avand doar unu dam skip la acest pas 
    -la al doilea pas, trei sa alegem un pod network add-on, si mai mult, cand rulam comanda $ kubeadm init (care ne va creea efectiv clsuteru) trebuie sa pasam si optiunea --pod-network-cidr ca sa ii zicem care va fi pod network-ul pe care il vom folosi, in cazu nsotru, vom folosi 10.224.0.0/16 
    -deci, pana acuma, comanda va arata asa:

    $ kubeadm init --pod-netowrk-cidr=10.224.0.0/16

    -la pasul 3, ne zica ca kubadm va incerca sa detecteze automat ce container runtime folosim. Daca nu reuseste sa faca asta, putem pasa optiunea --cri-socket si sa i dam pathu de la runtimeu nostru specific
    -dar, kubeadm va detecta automat ca folosim containerd, deci nu trei sa ne facem griji pt asta 
    -iar la pasu 4, care nu mai apare pe site acuma, cand rulam comanda kubeadm init, tre sa ii dam si --apiserver-advertise-address=2001:db8::101 , si asta va specifica efectiv care e adresa pe care serverul API o sa asculte si o sa o setam sa fie ip-ul static corect pe care l am setat pe master node, care va face serverul API accesibil pt toate worker nodes-urile pe acel ip specific 
    -deci, aici, doar ii dam ip-ul nodului nostru master pe care noi l am setat 
    -deci trei sa i dam aceste doua optiuni pod network cidr si apiserver advertise adress 
    -inainte sa facem asta, ne intoarcem aici https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ si mergem la Configuring a cgroup driver si dam pe linku cgroup driver, unde am mai fost cand am facut configurarile pt systemd 
    -am mentionat ca si kubelet trei sa fie setat la systemd, partea buna e ca dupa versiunea 1.22, cgroup driver este default systemd si fix asta vrem deci nu trei sa facem nimic diferit (Starting with v1.22 and later, when creating a cluster with kubeadm, if the user does not set the cgroupDriver field under KubeletConfiguration, kubeadm defaults it to systemd.)
    -daca folosim o versiune mai veche, trei sa modificam KubeletConfiguration, la optiunea cgroupDirver: systemd si cand rulam kubeadm init pasam optiunea --config si numele fileului editat: $ kubeadm init --config kubeadm config.yaml 
    -dar nu trei sa facem asta pt ca folosim systemd 
    -acuma ne intoarcem la pagina unde ramasesem,  https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ si rulam comanda kubeadm init cu cele doua flag-uri pe care le vrem. Dar facem asta doar pe master node, nu pe toate cele 3 nodes atentie!!!!
    
    -deci prima optiune o sa fie --pod-network-cidr=10.244.0.0/16 si a doua o sa fie --apiserver-advertise-address=192.168.56.2 , ip-u asta trebuie specificat, pt ca este IP-ul pe care serverul API va asculta si daca nu il setam, interfata network default va fi folosita, naiba stie care mai e si asa 
    -ca sa aflam a doua optiune, trei sa dam comanda $ ip add  si ne uitam mai jos la 3: enp0s8: si ne uitam la ce e dupa inet 10.180.48.33 deci asta o sa fie adresa ip a serverului API 
    -deci comanda va arata asa, mergem in nodul master numit controlplane si ii dam run cu root permission: $ sudo ...: 

    $ sudo kubeadm init --pod-network-cidr=10.224.0.0/16    // !!! TRE SA RULAM COMANDA ASTA FARA ADRESS ADVERTISE APARENT CA SA MEARGA ALTFEL ISI IA ERORI NASOALE 

    -asteptam unpic pana se termina si sa nu dam clear cumva la screem pt ca o sa avem nevoie de acesti pasi ca mai trei sa facem acuma cate ceva:
    -vedem ca scrie la un mom dat in logu comenzii aproape de final, To start your cluster, you need to run the fallowing as a regular user: ...
    -trei sa facem pasii aia sa pornim clusteru
    -primul lucru e ca un file admin.conf care a fost creeat pentru ca noi sa ne putem conecta la clusteru nostru kubernetes 
    -deci o sa creeam un folder .kube in home directory si va copia acest admin.conf file in acest folder, ca sa ne putem conecta la clusteru kubernetes si apoi o sa schimbam si niste permisiuni ca sa il putem folosi 
    -o sa luam aceste 3 comenzi: 

    $ mkdir -p $HOME/.kube
    $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    $ sudo chown $(id -u):$(id -g) $HOME/.kube/config

    -le rulam doar pe node-ul nostru master si acum ar trebui sa putem sa ne conectam la clusteru nostru 

    -tre sa dam comanda $ kubectl get pods ca sa vedem ca ne putem conecta la cluster deci am facut ok . tre sa ne dea ca nu s pods in workspace
    -daca nu mai merge si da eroare unable to connect dam comanda urm si facem iar initu:

    $ sudo kubeadm reset 

    -urm pas este sa dam deploy la pod network ca sa vedem toate network add-ons-urile pe care le putem folosi sa facem setup la pod network 
    -si dupa asta, mergem pe nodurile worker si dam comanda asta de la finalu optput ului comenzii de init: $ kubeadm join 10.0.2.15:6443 --token uejyf6.28vwvykqgk3jhouw --discovery-token-ca-cert-hash sha256:dd6731adbc20f65d2e2afda2c8bd7fda2db9f2751dd766d379c9ee29dceaf1eb
    -asta va da join la nodurile worker in clusteru kubernetes 

    -acuma dam click pe linku de deasupra comenzii de join https://kubernetes.io/docs/concepts/cluster-administration/addons/ 
    -aici avem o lista cu toate add-on-urile pe care le putem folosi, dar il folosim pe ultimu: Weave Net, mergem pe link-ul lui care ne trimite pe un repo github si daca dam mai jos avem comanda de instalare a weavenet pe clusteru nostru kubernetes:

    $ kubectl apply -f https://reweave.azurewebsites.net/k8s/v1.29/net.yaml




    -tot nu merge, o luam de la capat iar cu tutorialu de aicia https://github.com/kodekloudhub/certified-kubernetes-administrator-course/blob/master/kubeadm-clusters/generic/04-node-setup.md
    -putem da destroy la vm uri sau nu, dar trebe sa incepem cu comenzile astea orice varianta am alege-o 

    $ sudo apt-get update
    $ sudo apt-get install -y apt-transport-https ca-certificates curl

    {
        cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
    overlay
    br_netfilter
    EOF

        sudo modprobe overlay
        sudo modprobe br_netfilter
    }


    {
        cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
    net.bridge.bridge-nf-call-iptables  = 1
    net.bridge.bridge-nf-call-ip6tables = 1
    net.ipv4.ip_forward                 = 1
    EOF

        sudo sysctl --system
    }

    $ sudo apt-get install -y containerd

    {
        sudo mkdir -p /etc/containerd
        containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml
    }

    $ sudo systemctl restart containerd

    $ echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/${KUBE_LATEST}/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

    {
        sudo apt-get update
        sudo apt-get install -y kubelet kubeadm kubectl
        sudo apt-mark hold kubelet kubeadm kubectl
    }

    sudo crictl config \
    --set runtime-endpoint=unix:///run/containerd/containerd.sock \
    --set image-endpoint=unix:///run/containerd/containerd.sock

    cat <<EOF | sudo tee /etc/default/kubelet
    KUBELET_EXTRA_ARGS='--node-ip ${PRIMARY_IP}'
    EOF

    POD_CIDR=10.244.0.0/16
    SERVICE_CIDR=10.96.0.0/16

    sudo kubeadm init --pod-network-cidr $POD_CIDR --service-cidr $SERVICE_CIDR --apiserver-advertise-address $PRIMARY_IP

    {
        mkdir ~/.kube
        sudo cp /etc/kubernetes/admin.conf ~/.kube/config
        sudo chown $(id -u):$(id -g) ~/.kube/config
        chmod 600 ~/.kube/config
    }

    kubectl get pods -n kube-system

    kubectl apply -f "https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s-1.11.yaml"

    kubectl get pods -n kube-system

    ***

    The network used by the Virtual Box virtual machines is 192.168.56.0/24.  - PRIMARY IP 

    The network used to assign IP addresses to pods is 10.244.0.0/16.

    The network used to assign IP addresses to Cluster IP services is 10.96.0.0/16.
    
    PRIMARY_IP=192.168.56.0/24
    POD_CIDR=10.244.0.0/16
    SERVICE_CIDR=10.96.0.0/16

    ***

    -comanda pt a da join la workers 

    $ kubeadm join 192.168.56.11:6443 --token crhaar.reubilq23i6bcrt9 \
        --discovery-token-ca-cert-hash sha256:a18947957a6610a039837f269128cdd56d628f4278825369b11490f6a9fd7ee0

    kubeadm token create --print-join-command     // daca nu i am dat copy paste 

    - intram pe un worker node gen noed01 si dam urm comenzi: 

    $ sudo -i     // sa ne facem root si dupa dam paste la comanda urm 

    $ kubeadm join 192.168.56.11:6443 --token 4bckf3.hi8llixuitw6445f --discovery-token-ca-cert-hash sha256:a18947957a6610a039837f269128cdd56d628f4278825369b11490f6a9fd7ee0  

    -daca avem probleme de genu: [preflight] Running pre-flight checks
        error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
        [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
        To see the stack trace of this error execute with --v=5 or higher

    -dam urm comenzi:

    $ sudo kubeadm reset
    $ sudo rm -rf /etc/kubernetes/*
    $ sudo rm -rf /var/lib/etcd/*
    $ sudo rm -rf /var/lib/kubelet/*

    -apoi dam iar comanda de join

    -venim pe nodu master controlplane si dam comanda urm sa vedem ca a dat join workeru cu success:

    $ kubectl get nodes   //vedem ca e okei

    -tre sa copiem configuratia noastra kubernetes in controlplane sa o pornim de acolo pe worker nodes 
    -pt asta tre sai facem ssh cu coamanda scp 
    -pt asta ne trei pathu de la privatekey-ul lui controlplane care se afla aici: 

    E:\DevOps\Kubernetes\certified-kubernetes-administrator-course-master\kubeadm-clusters\virtualbox\.vagrant\machines\controlplane\virtualbox\private_key

    -facem un folder mkdir k8s in controlplane 
    -comanda de copy este, pe care o executam in folderu k8s-specifications din VotingApp:

    $ scp -i E:\DevOps\Kubernetes\certified-kubernetes-administrator-course-master\kubeadm-clusters\virtualbox\.vagrant\machines\controlplane\virtualbox\private_key worker-deployment.yaml vagrant@192.168.1.7:/home/vagrant/k8s/

    -executam comanda pt toate file-urile din k8s-specifications

    -acuma dam apply la aceste yaml uri:

    $ kubectl apply -f db-deployment.yaml

    -verificam pe ce nodes ruleaza pods-urile, dand comanda urm pe nodu controlplane(master): 

    $ kubectl get pods -o wide -A

    -asta ne confirma ca pods urile sunt inmpartite pe node urile noastre, deci scheduler isi face treaba
    -luam ip-ul nodului master cu comanda ip a si ne uitam la eps08 sau cv de genu si adaugam port urile dupa : 31000 su 31001 asa: http://192.168.1.8:31001
    -avem mai multe componente ale kubernetes: API server, etcd value store, kubeadm cli pt a seta clusteru cu maoster si workers, kubelet care e un serviciu care merge in spate cumva 


    -acuma o sa si testam unpica pe controlplane 

    $ kubectl create deployment nginx --image nginx:alpine
    $ kubectl expose deploy nginx --type=NodePort --port 80

    $ PORT_NUMBER=$(kubectl get service -l app=nginx -o jsonpath="{.items[0].spec.ports[0].nodePort}")
      echo -e "\n\nService exposed on NodePort $PORT_NUMBER"

    -pt a da hit la noul service 

    $ curl http://node01:$PORT_NUMBER
    $ curl http://node02:$PORT_NUMBER

    -daca vrem sa vedem din browser si am folosit BRIDGE Network driver, putem vedea cu serviciile NodePort din browseru nostru link urile 
    -dam cimanda urm in controlplane sa ne dea linku:

    $ echo "http://$(dig +short node01):$PORT_NUMBER"

    -ne da asta: http://192.168.56.21:30576 pt priumul woker node - node01 
    -ne da asta: http://192.168.56.22:30576 pt al doilea woker node - node02

Grafana and Prometheus Monitoring 

-ne asiguram ca merge minikube si ca avem deployment-urile facute 
-schimbam si imaginea de la kodekloud de la vode si result deployments cu a naostra david522/vote-app:vote si david522/vote-app:result

$ kubectl set image deployment/result-app-deploy result-app=david522/vote-app:result
$ kubectl set image deployment/vote-app-deploy vote-app=david522/vote-app:vote 

-acuma instalam helm prometheus si grafana 

helm:

$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash		# Install Helm on Ubuntu:

$ helm version

- Add the official Helm charts for Prometheus and Grafana:

$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts		

$ helm repo add grafana https://grafana.github.io/helm-charts

$ helm repo update

kubectl create namespace monitoring			# optinal dar decomandat 

- Install Prometheus using Helm: 

$ helm install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring

$ kubectl get pods -n monitoring		# verificam statusu deployment-ului prometheus si asteptam ca toate pods-urile prometheus sa fie running 

- Install Grafana Using Helm

$ helm install grafana grafana/grafana --namespace monitoring

$ kubectl get pods -n monitoring      # verificam statusu deployment-ului grafana 

-ca sa accesam dashboard-urile grafana si prometheus trei sa facem port forwarding sa accesam grafana:

$ kubectl port-forward service/grafana 3000:80 -n monitoring		# so we can access the grafana web ui at : localhost:3000 

$ kubectl port-forward svc/prometheus-kube-prometheus-prometheus -n monitoring 9090:9090	# for accessing the prometheus web ui 


$ kubectl get secret --namespace monitoring grafana -o jsonpath="{.data.admin-password}" | base64 --decode		# ca sa aflam parola pt grafana ca nu merge admin. Asta e: Xda2pSwSmwXB0Dpf7hqxO2kkcYT6dV7Pjrsm4odl

- Go to Configuration (gear icon) > Data Sources > Add data source. Choose Prometheus from the list.
- In the URL field, enter the Prometheus service URL (http://prometheus-operated:9090). Click Save & Test to verify the connection.
- Go to Create > Import in the Grafana UI. Enter the Dashboard ID (e.g., 3119, 315, 3131, 6417, 8171, 741 for Kubernetes monitoring) and click Load. Select the Prometheus data source and click Import.

$ minikube dashboard 		# dashboard 
























